<!DOCTYPE html>



  



<script type="text/javascript"
color="0,0,255" opacity='0.7' zIndex="-2" count="99" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script>


<html class="theme-next pisces use-motion" lang="zh-cn">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="-TensorFlow," />





  <link rel="alternate" href="/atom.xml" title="Yang_1998's Blog" type="application/atom+xml" />






<meta name="description" content="PPT搬运工(而且还搬不全)">
<meta name="keywords" content="-TensorFlow">
<meta property="og:type" content="article">
<meta property="og:title" content="TensorFlow">
<meta property="og:url" content="http://yoursite.com/2019/01/09/tensorflow_review/index.html">
<meta property="og:site_name" content="Yang_1998&#39;s Blog">
<meta property="og:description" content="PPT搬运工(而且还搬不全)">
<meta property="og:locale" content="zh-cn">
<meta property="og:updated_time" content="2019-01-09T08:50:45.640Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TensorFlow">
<meta name="twitter:description" content="PPT搬运工(而且还搬不全)">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/01/09/tensorflow_review/"/>





  <title>TensorFlow | Yang_1998's Blog</title>
  








</head>
<body itemscope itemtype="http://schema.org/WebPage" lang="zh-cn">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband">
    	<a href="https://github.com/yangzixu666" class="github-corner" aria-label="View source on Github"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    </div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Yang_1998's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">王鹏赫你在哪呢</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-首页">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-关于我">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于我
          </a>
        </li>
      
        
        <li class="menu-item menu-item-标签">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-分类">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/01/09/tensorflow_review/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yangzixu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/Head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yang_1998's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">TensorFlow</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-01-09T16:48:30+08:00">
                2019-01-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/学习笔记/" itemprop="url" rel="index">
                    <span itemprop="name">-"学习笔记"</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2019/01/09/tensorflow_review/" class="leancloud_visitors" data-flag-title="TensorFlow">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"><span>℃</span></span>
                 
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  17,478
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  75
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><strong>PPT搬运工(而且还搬不全)</strong></p>
<a id="more"></a>
<h1 id="1-TensorFlow概述"><a href="#1-TensorFlow概述" class="headerlink" title="1.TensorFlow概述"></a>1.TensorFlow概述</h1><h3 id="Tensorflow是什么"><a href="#Tensorflow是什么" class="headerlink" title="Tensorflow是什么?"></a>Tensorflow是什么?</h3><ul>
<li>Tensorflow是由Google Brain Team开发的使用<strong>数据流图</strong>进行数值计算的开源机器学习库</li>
<li>Tensorflowd的一大亮点是支持异构设备分布式计算.这里<strong>异构设备是指使用CPU,GPU等计算设备进行有效地合作</strong></li>
</ul>
<h3 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h3><ul>
<li>Tensor是<strong>张量</strong>的意思,原本在物理理学中用用来描述大大于等于2维的量量进行行行量量纲分析的工工具.0维的量(纯量)、1维的量(向量)、2维的量(矩阵).对于高高维的数据,我们也需要一个工工具来表述,这个工工具正是张量量。</li>
<li>张量量类似于编程语言言中的多维数组(或列列表)。广广义的张量量包括了了常量量、向量量、矩阵以及高高维数据。</li>
</ul>
<h3 id="Flow"><a href="#Flow" class="headerlink" title="Flow"></a>Flow</h3><ul>
<li>flow是“流”的意思,这里里里可以可以理理解为数据的流动。Tensorflow所表达的意思就是“<strong>张量流</strong>”。</li>
</ul>
<h3 id="编程模式"><a href="#编程模式" class="headerlink" title="编程模式"></a>编程模式</h3><ul>
<li>编程模式通常分为<strong>命令式编程</strong>(imperative style programs)和<strong>符号式编程</strong>(symbolic style<br>programs)</li>
<li>命令式编程模式:直接执行逻辑语句完成相应任务,容易理解和调试</li>
<li>符号式编程模式:涉及较多的嵌入和优化,很多任务中的逻辑需要使用图进行表示,并在其他语言环境中执行完成,不容易理解和调试,但运行速度有同比提升</li>
<li>机器学习中,大部分现代的框架使用符号式编程,原因是编写程序容易且运行速度快</li>
</ul>
<p><strong>对于一个a+b问题来说</strong></p>
<p><strong>命令式编程:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a=np.ones([<span class="number">10</span>,])</span><br><span class="line">b=np.ones([<span class="number">10</span>,])*<span class="number">5</span></span><br><span class="line">c=a+b</span><br></pre></td></tr></table></figure>
<p><strong>符号式编程:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = Ones_Variables(<span class="string">'A'</span>, shape=[<span class="number">10</span>,])</span><br><span class="line">b = Ones_Variables(<span class="string">'B'</span>, shape=[<span class="number">10</span>,])</span><br><span class="line">c = Add(a, b)</span><br><span class="line"></span><br><span class="line">Run(c)</span><br></pre></td></tr></table></figure>
<p>上述代码执行到c=Add(a, b)时,并不会真正的执行加法运算,同样的a、b也并没有对应的数值,a、b、c均是一个符号,符号定义了执行运算的结构,我们称之为计算图,计算图没有执行真正的运算。当执行Run(c)时,计算图开始真正的执行计算,计算的环境通常不是当前的语言环境,而而是C++等效率更高的语言环境</p>
<h3 id="数据流图"><a href="#数据流图" class="headerlink" title="数据流图"></a>数据流图</h3><p><strong>当我们使用用计算图来表示计算过程时,事实上可以看做是一个推断过程,在推断时,我们输入一些数据,并使用符号来表示各种计算过程,最终得到一个或多个推断结果。所以使用用计算图可以在一定程度上对计算结果进行预测。</strong></p>
<h1 id="2-图与绘画"><a href="#2-图与绘画" class="headerlink" title="2.图与绘画"></a>2.图与绘画</h1><h3 id="数据流图-1"><a href="#数据流图-1" class="headerlink" title="数据流图"></a>数据流图</h3><ul>
<li>Tensorflow使用<strong>数据流图</strong>来定义计算流程.数据流图阶段<strong>并不会执行处计算结果</strong>.数据流图使得计算的<strong>定义</strong>与<strong>执行</strong>分离开来.Tensorflow构建数据流图是有向无环图.图的执行需要在<strong>会话(Session)</strong>中完成</li>
<li><p>数据流图包含了边(edge)和节点(node),正好与tensor和flow对应.<strong>tensor代表了数据流图中的边</strong>,<strong>flow代表数据流图中的节点</strong></p>
</li>
<li><p>节点也被称为<strong>操作(operation,op)</strong>,一个 op 获得 0 个或多个tensor,执行计算,产生0个或多个tensor</p>
</li>
</ul>
<h3 id="会话"><a href="#会话" class="headerlink" title="会话"></a>会话</h3><ul>
<li><p>启动会话的第一步是创建一个session对象.会话提供在图中执行op的一系列方法.一般模式是,建立会话,在会话中添加图,然后执行.</p>
</li>
<li><p>在会话中执行图</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment">#创建图</span></span><br><span class="line">a=tf.add(<span class="number">3</span>,<span class="number">5</span>)</span><br><span class="line"><span class="comment">#创建会话</span></span><br><span class="line">sess=tf.Session()</span><br><span class="line"><span class="comment">#执行图</span></span><br><span class="line">res=sess.run(a)</span><br><span class="line"><span class="comment">#关闭会话</span></span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>
</li>
<li><p>为了简便,我们可以使用上下文管理器来创建Session,所以也可以写成</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a=tf.add(<span class="number">3</span>,<span class="number">5</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(a))</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="feed与fetch"><a href="#feed与fetch" class="headerlink" title="feed与fetch"></a>feed与fetch</h3><ul>
<li>在调用Session对象的run()方法来执行图时，传入一些张量，这一过程叫做<strong>填充（feed）</strong>，返回的结果类型根据输入的类型而定，这个过程叫<strong>取回（fetch）</strong>。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">run(fetches,feed_dict=<span class="keyword">None</span>,options=<span class="keyword">None</span>,run_metadata=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><p>除了使用tf.Session.run()以外,在sess持有的上下文中还可以使用eval()方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a=tf.add(<span class="number">3</span>,<span class="number">5</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(a.eval())</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a=tf.add(<span class="number">3</span>,<span class="number">5</span>)</span><br><span class="line">sess=tf.InteractiveSession()</span><br><span class="line">print(a.eval())</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>注意</strong>：当我们构建一个加法运算的图，如<code>tf.add(3, 5)</code>时，这时候如果打印其返回值，可以看到返回值为一个<code>Tensor</code>对象，而将<code>Tensor</code>对象填充进入会话中执行之后，取回的值并不是<code>Tensor</code>对象，而是<code>numpy</code>下的<code>ndarray</code>对象。这是因为<code>TensorFlow</code>底层使用了<code>numpy</code>作为科学计算核心，而<code>Tensor</code>对象仅仅是一个符号，没有具体的数值，在会话中执行完成之后返回的<code>ndarray</code>对象是包含具体值的。</p>
</li>
</ul>
<h3 id="节点依赖"><a href="#节点依赖" class="headerlink" title="节点依赖"></a>节点依赖</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">x=<span class="number">2</span></span><br><span class="line">y=<span class="number">3</span></span><br><span class="line">op1=tf.add(x,y)</span><br><span class="line">op2=tf.multiply(x,y)</span><br><span class="line">op3=tf.pow(op2,op1)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    res=sess.run(op3)</span><br><span class="line">    print(res)</span><br><span class="line">    </span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">虽然`session`中运行的是节点`op3`，然而与之相关联的`op1`、`op2`也参与了运算。</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<p>小练习:</p>
<p>构建一个图.描述从1到100的累加,并在会话中执行得到结果.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment">#定义变量</span></span><br><span class="line">a=tf.Variable(<span class="number">0</span>)</span><br><span class="line">ans=tf.Variable(<span class="number">0</span>)</span><br><span class="line"><span class="comment">#定义加法</span></span><br><span class="line">Add=tf.add(a,<span class="number">1</span>)</span><br><span class="line"><span class="comment">#更新变量</span></span><br><span class="line">update=tf.assign(a,Add)</span><br><span class="line"><span class="comment">#求和</span></span><br><span class="line">Add2=tf.add(ans,update)</span><br><span class="line">Sum=tf.assign(ans,Add2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">        print(sess.run(Sum))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment">#定义变量</span></span><br><span class="line">a=tf.Variable(<span class="number">0</span>)</span><br><span class="line">ans=tf.Variable(<span class="number">0</span>)</span><br><span class="line"><span class="comment">#定义加法</span></span><br><span class="line">Add=tf.assign_add(a,<span class="number">1</span>)</span><br><span class="line"><span class="comment">#求和</span></span><br><span class="line">Sum=tf.assign_add(ans,Add)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">        print(sess.run(Sum))</span><br></pre></td></tr></table></figure>
<h3 id="子图"><a href="#子图" class="headerlink" title="子图"></a>子图</h3><p>图的构建可以是多种多样的，以上构建的图中数据只有一个流动方向，事实上我们可以构建多个通路的图，每一个通路可以称之为其子图。</p>
<h3 id="多个图"><a href="#多个图" class="headerlink" title="多个图"></a>多个图</h3><p>创建图使用<code>tf.Graph()</code>。将图设置为默认图使用<code>tf.Graph.as_default()</code>，此方法会返回一个上下文管理器。如果不显示的添加一个默认图，系统会自动设置一个全局的默认图。所设置的默认图，在模块范围内定义的节点都将自动加入默认图中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#构建一个图</span></span><br><span class="line">g=tf.Graph()</span><br><span class="line"><span class="comment">#将此图作为默认图</span></span><br><span class="line"><span class="keyword">with</span> g.as_default():</span><br><span class="line">    op=tf.add(<span class="number">3</span>,<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p>改写:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Graph.as_default():</span><br><span class="line">    op=tf.add(<span class="number">3</span>,<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p>构建多个图:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">g1=tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g1.as_default():</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">g2=tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g2.as_default():</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>在我们加载了Tensorflow包时，就已经加载了一个图，这也是我们可以不创建图而直接构建边和节点的原因，这些边与节点均加入了默认图。我们可以使用<code>tf.get_default_graph()</code>方法获取到当前环境中的图，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取到了系统的默认图</span></span><br><span class="line">g1 = tf.get_default_graph()</span><br><span class="line"><span class="keyword">with</span> g1.as_default():</span><br><span class="line">	<span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">g2 = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g2.as_default():</span><br><span class="line">	<span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>当使用了多个图时，在<code>session</code>中就需要指定当前<code>session</code>所运行的图，不指定时，为默认图。如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session(graph=g) <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(op)</span><br></pre></td></tr></table></figure>
<h3 id="单机多卡计算"><a href="#单机多卡计算" class="headerlink" title="单机多卡计算"></a>单机多卡计算</h3><p>Tensorflow能自动检测，如果检测到GPU，Tensorflow会尽可能的利用找到的第一个GPU进行操作。如果机器上有超过一个可用的GPU，出第一个外，其它的GPU默认是不参与计算的。为了让Tensorflow使用这些GPU，我们必须明确指定使用何种设备进行计算。</p>
<p>TensorFlow用指定字符串 <code>strings</code> 来标识这些设备. 比如:</p>
<ul>
<li><code>&quot;/cpu:0&quot;</code>: 机器中的 CPU</li>
<li><code>&quot;/gpu:0&quot;</code>: 机器中的 GPU, 如果你有一个的话.</li>
<li><code>&quot;/gpu:1&quot;</code>: 机器中的第二个 GPU, 以此类推…</li>
</ul>
<p>这里需要注意的是，<strong>CPU</strong>可以<strong>支持所有类型的</strong>运行，而<strong>GPU</strong>只<strong>支持部分运算</strong>，例如矩阵乘法。下面是一个使用第3个GPU进行矩阵乘法的运算：</p>
<p>例如:使用第3个GPU进行矩阵乘法的运算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/gpu:2'</span>):</span><br><span class="line">	a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>]], name=<span class="string">'a'</span>)</span><br><span class="line">    b = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>], [<span class="number">5.0</span>, <span class="number">6.0</span>]], name=<span class="string">'b'</span>)</span><br><span class="line">  	c = tf.matmul(a, b)</span><br></pre></td></tr></table></figure>
<p>为了查看节点和边被指派给哪个设备运算，可以在<code>session</code>中配置“记录设备指派”为开启，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session(config=tf.ConfigProto(log_device_placement=<span class="keyword">True</span>)) <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(c))</span><br></pre></td></tr></table></figure>
<p>当指定的设备不存在时，会出现 <code>InvalidArgumentError</code> 错误提示。为了避免指定的设备不存在这种情况，可以在创建的 <code>session</code> 里把参数 <code>allow_soft_placement</code> 设置为 <code>True</code>，这时候TF会在设备不存在时自动分配设备。如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">config = tf.ConfigProto(</span><br><span class="line">	log_device_placement=<span class="keyword">True</span>,</span><br><span class="line">	allow_soft_placement=<span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session(config=config) <span class="keyword">as</span> sess:</span><br><span class="line">	<span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<h3 id="使用图与会话的优点"><a href="#使用图与会话的优点" class="headerlink" title="使用图与会话的优点"></a>使用图与会话的优点</h3><p>对于图与会话：</p>
<ul>
<li>多个图需要多个会话来运行，每个会话会尝试使用所有可用的资源。</li>
<li>不同图、不同会话之间无法直接通信。</li>
<li>最好在一个图中有多个不连贯的子图。</li>
</ul>
<p>使用图的优点：</p>
<ul>
<li>节省计算资源。我们仅仅只需要计算用到的子图就可以了，无需完整计算整个图。</li>
<li>可以将复杂计算分成小块，进行自动微分等运算。</li>
<li>促进分布式计算，将工作分布在多个CPU、GPU等设备上。</li>
<li>很多机器学习模型可以使用有向图进行表示，与计算图一致。</li>
</ul>
<p><a href="https://github.com/m-L-0/18a-YangZixu-2016-533/tree/master/Second" target="_blank" rel="noopener">作业传送门</a></p>
<h1 id="3-图的边与节点"><a href="#3-图的边与节点" class="headerlink" title="3.图的边与节点"></a>3.图的边与节点</h1><h3 id="边-edge"><a href="#边-edge" class="headerlink" title="边(edge)"></a>边(edge)</h3><p>Tensorflow的边有两种连接关系：<strong>数据依赖</strong>和<strong>控制依赖</strong>。其中，<strong>实线边表示数据依赖</strong>，<strong>代表数据，即张量</strong>。<strong>虚线边表示控制依赖</strong>（control dependency），可用于控制操作的运行，这被用来确保happens-before关系，这类边上没有数据流过，但源节点必须在目的节点开始执行前完成执行。</p>
<h3 id="数据依赖"><a href="#数据依赖" class="headerlink" title="数据依赖"></a>数据依赖</h3><p>数据依赖很容易理解，某个节点会依赖于其它节点的数据，如下所示，矩阵乘法操作这个节点依赖于<code>a、b</code>的数据才能执行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a=tf.constant([[1.0,2.0,3.0],[4.0,5.0,6.0]],name=&apos;a&apos;)</span><br><span class="line">b=tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], name=&apos;b&apos;)</span><br><span class="line">c=tf.matmul(a,b)</span><br></pre></td></tr></table></figure>
<p>当节点关系比较复杂时，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>]], name=<span class="string">'a'</span>)</span><br><span class="line">b = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>], [<span class="number">5.0</span>, <span class="number">6.0</span>]], name=<span class="string">'b'</span>)</span><br><span class="line">c = tf.matmul(a, b)</span><br><span class="line">d = tf.add(c, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<h3 id="控制依赖"><a href="#控制依赖" class="headerlink" title="控制依赖"></a>控制依赖</h3><p>控制依赖是在某些操作之间没有数值上的依赖关系但执行时又需要使这些操作按照一定顺序执行，这时候我们可以声明执行顺序。这在TensorFlow包含变量相关操作时非常常用。</p>
<p>控制依赖使用<strong>图对象的方法</strong><code>tf.Graph.control_dependencies(control_inputs)</code>，返回一个可以使用上下文管理器的对象，用法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>]], name=<span class="string">'a'</span>)</span><br><span class="line">b = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>], [<span class="number">5.0</span>, <span class="number">6.0</span>]], name=<span class="string">'b'</span>)</span><br><span class="line">c = tf.matmul(a, b)</span><br><span class="line"></span><br><span class="line">g = tf.get_default_graph()</span><br><span class="line"><span class="keyword">with</span> g.control_dependencies([c]):</span><br><span class="line">    d = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>]], name=<span class="string">'d'</span>)</span><br><span class="line">    e = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>], [<span class="number">5.0</span>, <span class="number">6.0</span>]], name=<span class="string">'e'</span>)</span><br><span class="line">    f = tf.matmul(d, e)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(f)</span><br></pre></td></tr></table></figure>
<p>上面的例子中，我们在会话中执行了<code>f</code>这个节点，可以看到其与<code>c</code>这个节点并无任何数据依赖关系，然而<code>f</code>这个节点必须等待<code>c</code>这个节点执行完成才能够执行<code>f</code>。最终的结果是<code>c</code>先执行，<code>f</code>再执行</p>
<p><strong>注意</strong>：<code>control_dependencies</code>方法传入的是一个列表作为参数，列表中包含所有被依赖的操作或张量，被依赖的所有节点可以看做是同时执行的。</p>
<h3 id="张量的阶、形状、数据类型"><a href="#张量的阶、形状、数据类型" class="headerlink" title="张量的阶、形状、数据类型"></a>张量的阶、形状、数据类型</h3><p>Tensorflow数据流图中的边用于数据传输时，数据是以张量的形式传递的。张量有<strong>阶</strong>、<strong>形状</strong>和<strong>数据类型</strong>等属性。</p>
<h4 id="Tensor的阶"><a href="#Tensor的阶" class="headerlink" title="Tensor的阶"></a><strong>Tensor的阶</strong></h4><p>在TensorFlow系统中，张量的维数被描述为<strong>阶</strong>。但是张量的阶和矩阵的阶并不是同一个概念。张量的阶是张量维数的一个数量描述。比如，下面的张量（使用Python中list定义的）就是2阶.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]</span><br></pre></td></tr></table></figure>
<p>你可以认为一个二阶张量就是我们平常所说的矩阵，一阶张量可以认为是一个向量.对于一个二阶张量你可以用语句<code>t[i, j]</code>来访问其中的任何元素.而对于三阶张量你可以用<code>t[i, j, k]</code>来访问其中的任何元素。</p>
<p>张量的阶可以使用<code>tf.rank()</code>获取到：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>]])</span><br><span class="line">tf.rank(a)  <span class="comment"># &lt;tf.Tensor 'Rank:0' shape=() dtype=int32&gt; =&gt; 2</span></span><br></pre></td></tr></table></figure>
<p>张量的形状可以通过Python中的列表或元祖（list或tuples）来表示，或者也可用<code>TensorShape</code>对象来表示。如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定shape是[2, 3]的常量,这里使用了list指定了shape，也可以使用ndarray和TensorShape对象来指定shape</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>]], shape=[<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取shape 方法一：利用tensor的shape属性</span></span><br><span class="line">a.shape  <span class="comment"># TensorShape([Dimension(2), Dimension(3)])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取shape 方法二：利用Tensor的方法get_shape()</span></span><br><span class="line">a.get_shape()  <span class="comment"># TensorShape([Dimension(2), Dimension(3)])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取shape 方法三：利用tf.shape()</span></span><br><span class="line">tf.shape(a) <span class="comment"># &lt;tf.Tensor 'Shape:0' shape=(2,) dtype=int32&gt;</span></span><br></pre></td></tr></table></figure>
<p><code>TensorShape</code>对象有一个方法<code>as_list()</code>，可以将<code>TensorShape</code>对象转化为python的list对象。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.get_shape().as_list() <span class="comment"># [2, 3]</span></span><br></pre></td></tr></table></figure>
<p>小练习</p>
<ol>
<li>写出如下张量的阶与形状，并使用TensorFlow编程验证：</li>
</ol>
<ul>
<li><code>[]</code>   <strong>rank:1 shape:(0,)</strong></li>
<li><code>12</code>    <strong>rank:0 shape:()</strong></li>
<li><code>[[1], [2], [3]]</code>    <strong>rank:2 shape:(3,1)</strong></li>
<li><code>[[1, 2, 3], [1, 2, 3]]</code>  <strong>rank:2 shape:(2,3)</strong></li>
<li><code>[[[1]], [[2]], [[3]]]</code>   <strong>rank:3 shape:(3,1,1)</strong></li>
<li><code>[[[1, 2], [1, 2]]]</code>      <strong>rank:3 shape:(1,2,2)</strong></li>
</ul>
<p>设计一个函数，要求实现：可以根据输入张量输出shape完成一样的元素为全1的张量。提示，使用<code>tf.ones</code>函数可根据形状生成全1张量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">solve</span><span class="params">(Shape)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.ones(Shape)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(solve(a.shape)))</span><br></pre></td></tr></table></figure>
<p><strong>设置与获取Tensor的数据类型</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法一</span></span><br><span class="line"><span class="comment"># Tensorflow会推断出类型为tf.float32</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法二</span></span><br><span class="line"><span class="comment"># 手动设置</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>]], dtype=tf.float32)</span><br></pre></td></tr></table></figure>
<p><strong>获取Tensor的数据类型,可以使用如下方法</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>]], name=<span class="string">'a'</span>)</span><br><span class="line">a.dtype  <span class="comment"># tf.float32</span></span><br><span class="line">print(a.dtype)  <span class="comment"># &gt;&gt; &lt;dtype: 'float32'&gt;</span></span><br><span class="line"></span><br><span class="line">b = tf.constant(<span class="number">2</span>+<span class="number">3j</span>)  <span class="comment"># tf.complex128 等价于 tf.complex(2., 3.)</span></span><br><span class="line">print(b.dtype)  <span class="comment"># &gt;&gt; &lt;dtype: 'complex128'&gt;</span></span><br><span class="line"></span><br><span class="line">c = tf.constant([<span class="keyword">True</span>, <span class="keyword">False</span>], tf.bool)</span><br><span class="line">print(c.dtype)  <span class="comment"># &lt;dtype: 'bool'&gt;</span></span><br></pre></td></tr></table></figure>
<p><strong>这里需要注意的是一个张量仅允许一种dtype存在，也就是一个张量中每一个数据的数据类型必须一致。</strong></p>
<p>数据类型转化</p>
<p>如果我们需要将一种数据类型转化为另一种数据类型，需要使用<code>tf.cast()</code>进行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>]], name=<span class="string">'a'</span>)</span><br><span class="line"><span class="comment"># tf.cast(x, dtype, name=None) 通常用来在两种数值类型之间互转</span></span><br><span class="line">b = tf.cast(a, tf.int16)</span><br><span class="line">print(b.dtype)  <span class="comment"># &gt;&gt; &lt;dtype: 'int16'&gt;</span></span><br></pre></td></tr></table></figure>
<p>有些类型利用<code>tf.cast()</code>是无法互转的，比如string无法转化成为number类型，这时候可以使用以下方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将string转化为number类型 注意：数字字符可以转化为数字</span></span><br><span class="line"><span class="comment"># tf.string_to_number(string_tensor, out_type = None, name = None)</span></span><br><span class="line">a = tf.constant([[<span class="string">'1.0'</span>, <span class="string">'2.0'</span>, <span class="string">'3.0'</span>], [<span class="string">'4.0'</span>, <span class="string">'5.0'</span>, <span class="string">'6.0'</span>]], name=<span class="string">'a'</span>)</span><br><span class="line">num = tf.string_to_number(a)</span><br></pre></td></tr></table></figure>
<p><strong>实数数值类型可以使用<code>tf.cast</code>方法转化为bool类型。</strong></p>
<h3 id="节点"><a href="#节点" class="headerlink" title="节点"></a>节点</h3><p>图中的节点也可以称之为<strong>算子</strong>，它代表一个操作(operation, OP)，一般用来表示数学运算，也可以表示数据输入（feed in）的起点以及输出（push out）的终点，或者是读取/写入持久变量（persistent variable）的终点。常见的节点包括以下几种类型：<strong>变量、张量逐元素运算、张量变形、张量索引与切片、张量运算、检查点操作、队列和同步操作、张量</strong>控制等。</p>
<p>当OP表示数学运算时，每一个运算都会创建一个<code>tf.Operation</code>对象。常见的操作，例如生成一个变量或者常量、数值计算均创建<code>tf.Operation</code>对象</p>
<h3 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h3><p>变量用于存储张量,可以使用list,Tensor等来进行初始化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">var=tf.Variable(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h3 id="张量运算"><a href="#张量运算" class="headerlink" title="张量运算"></a>张量运算</h3><p>加法:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.add(<span class="number">1</span>,<span class="number">2</span>) <span class="comment">#3</span></span><br></pre></td></tr></table></figure>
<p>减法:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.subtract(<span class="number">1</span>,<span class="number">2</span>) <span class="comment">#-1</span></span><br><span class="line">tf.subtract([<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>])  <span class="comment"># [-2, -2]</span></span><br><span class="line">tf.constant([<span class="number">1</span>, <span class="number">2</span>]) - tf.constant([<span class="number">3</span>, <span class="number">4</span>])  <span class="comment"># [-2, -2]</span></span><br></pre></td></tr></table></figure>
<p>乘法:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.multiply(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># 2</span></span><br><span class="line">tf.multiply([<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>])  <span class="comment"># [3, 8]</span></span><br><span class="line">tf.constant([<span class="number">1</span>, <span class="number">2</span>]) * tf.constant([<span class="number">3</span>, <span class="number">4</span>])  <span class="comment"># [3, 8]</span></span><br></pre></td></tr></table></figure>
<p><code>tf.scalar_mul()</code>一个纯量分别与张量中每一个元素相乘。等价于 <code>a * B</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.run(tf.scalar_mul(<span class="number">10.</span>, tf.constant([<span class="number">1.</span>, <span class="number">2.</span>])))  <span class="comment"># [10., 20.]</span></span><br></pre></td></tr></table></figure>
<p><code>tf.divide()</code> 两个张量对应元素相除。等价于<code>A / B</code>。这个除法操作是Tensorflow推荐使用的方法。此方法不接受Python自身的数据结构，例如常量或list等。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.divide(tf.constant([<span class="number">1</span>, <span class="number">2</span>]), tf.constant([<span class="number">3</span>, <span class="number">4</span>]))  <span class="comment"># [0.33333333, 0.5]</span></span><br></pre></td></tr></table></figure>
<p><code>tf.floordiv()</code> shape相同的两个张量对应元素相除取整数部分。等价于<code>A // B</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.floordiv(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># 0</span></span><br><span class="line">tf.floordiv([<span class="number">4</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">5</span>])  <span class="comment"># [2, 0]</span></span><br><span class="line">tf.constant([<span class="number">4</span>, <span class="number">3</span>]) // tf.constant([<span class="number">2</span>, <span class="number">5</span>])  <span class="comment"># [2, 0]</span></span><br></pre></td></tr></table></figure>
<p><code>tf.mod()</code> shape相同的两个张量对应元素进行模运算。等价于<code>A % B</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tf.mod([<span class="number">4</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">5</span>])  <span class="comment"># [0, 3]</span></span><br><span class="line">tf.constant([<span class="number">4</span>, <span class="number">3</span>]) % tf.constant([<span class="number">2</span>, <span class="number">5</span>])  <span class="comment"># [0, 3]</span></span><br></pre></td></tr></table></figure>
<h3 id="张量常用运算"><a href="#张量常用运算" class="headerlink" title="张量常用运算"></a>张量常用运算</h3><p><code>tf.matmul()</code> 通常用来做矩阵乘法。</p>
<p><code>tf.transpose()</code> 转置张量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>], [<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.0</span>]])</span><br><span class="line"><span class="comment"># tf.matmul(a, b, transpose_a=False, transpose_b=False, adjoint_a=False, adjoint_b=False, a_is_sparse=False, b_is_sparse=False, name=None)</span></span><br><span class="line"><span class="comment"># tf.transpose(a, perm=None, name='transpose')</span></span><br><span class="line">tf.matmul(a, tf.transpose(a))  <span class="comment"># 等价于 tf.matmul(a, a, transpose_b=True)</span></span><br></pre></td></tr></table></figure>
<h3 id="张量切片与索引"><a href="#张量切片与索引" class="headerlink" title="张量切片与索引"></a>张量切片与索引</h3><p>张量的变形:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将张量变为指定shape的新张量</span></span><br><span class="line"><span class="comment"># tf.reshape(tensor, shape, name=None)</span></span><br><span class="line"><span class="comment"># tensor 't' is [1, 2, 3, 4, 5, 6, 7, 8, 9]</span></span><br><span class="line"><span class="comment"># tensor 't' has shape [9]</span></span><br><span class="line">new_t = tf.reshape(t, [<span class="number">3</span>, <span class="number">3</span>]) </span><br><span class="line"><span class="comment"># new_t	==&gt; [[1, 2, 3],</span></span><br><span class="line"><span class="comment">#            [4, 5, 6],</span></span><br><span class="line"><span class="comment">#            [7, 8, 9]]</span></span><br><span class="line">new_t = tf.reshape(new_t, [<span class="number">-1</span>]) <span class="comment"># 这里需要注意shape是一阶张量，此处不能直接使用 -1</span></span><br><span class="line"><span class="comment"># tensor 'new_t' is [1, 2, 3, 4, 5, 6, 7, 8, 9]</span></span><br></pre></td></tr></table></figure>
<p>张量的拼接：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 沿着某个维度对二个或多个张量进行连接</span></span><br><span class="line"><span class="comment"># tf.concat(values, axis, name='concat')</span></span><br><span class="line">t1 = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]</span><br><span class="line">t2 = [[<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]]</span><br><span class="line">tf.concat([t1, t2], 0) ==&gt; [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]</span><br></pre></td></tr></table></figure>
<p>张量的切割：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对输入的张量进行切片</span></span><br><span class="line"><span class="comment"># tf.slice(input_, begin, size, name=None)</span></span><br><span class="line"><span class="comment"># 'input' is [[[1, 1, 1], [2, 2, 2]],</span></span><br><span class="line"><span class="comment">#             [[3, 3, 3], [4, 4, 4]],</span></span><br><span class="line"><span class="comment">#             [[5, 5, 5], [6, 6, 6]]]</span></span><br><span class="line">tf.slice(input, [1, 0, 0], [1, 1, 3]) ==&gt; [[[3, 3, 3]]]</span><br><span class="line">tf.slice(input, [1, 0, 0], [1, 2, 3]) ==&gt; [[[3, 3, 3],</span><br><span class="line">                                            [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]]]</span><br><span class="line">tf.slice(input, [1, 0, 0], [2, 1, 3]) ==&gt; [[[3, 3, 3]],</span><br><span class="line">                                           [[<span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>]]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将张量分裂成子张量</span></span><br><span class="line"><span class="comment"># tf.split(value, num_or_size_splits, axis=0, num=None, name='split')</span></span><br><span class="line"><span class="comment"># 'value' is a tensor with shape [5, 30]</span></span><br><span class="line"><span class="comment"># Split 'value' into 3 tensors with sizes [4, 15, 11] along dimension 1</span></span><br><span class="line">split0, split1, split2 = tf.split(value, [<span class="number">4</span>, <span class="number">15</span>, <span class="number">11</span>], <span class="number">1</span>)</span><br><span class="line">tf.shape(split0) ==&gt; [5, 4]</span><br><span class="line">tf.shape(split1) ==&gt; [5, 15]</span><br><span class="line">tf.shape(split2) ==&gt; [5, 11]</span><br></pre></td></tr></table></figure>
<p>有一4阶张量<code>img</code>其<code>shape=[10, 28, 28, 3])</code>，代表10张28*28像素的3通道RGB图像，问：</p>
<ol>
<li>如何利用索引取出第2张图片？（注意：索引均从0开始，第二张则索引为1，下同）</li>
<li>如何利用切片取出第2张图片？</li>
<li>使用切片与使用索引取出的一张图片有何不同？</li>
<li>如何取出其中的第1、3、5、7张图片？</li>
<li>如何取出第6-8张（包括6不包括8）图片中中心区域（14*14）的部分？</li>
<li>如何将图片根据通道拆分成三份单通道图片？</li>
<li>写出<code>tf.shape(img)</code>返回的张量的阶数以及<code>shape</code>属性的值。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">a=tf.ones([<span class="number">10</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">3</span>])</span><br><span class="line"><span class="comment">#如何利用索引取出第2张图片</span></span><br><span class="line">b=a[<span class="number">1</span>:<span class="number">2</span>,:,:,:]</span><br><span class="line"><span class="comment">#如何利用切片取出第2张图片？</span></span><br><span class="line">c=tf.slice(a,[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">3</span>])</span><br><span class="line"><span class="comment">#如何取出其中的第1、3、5、7张图片</span></span><br><span class="line">d=a[<span class="number">0</span>:<span class="number">7</span>:<span class="number">2</span>,:,:,:]</span><br><span class="line"><span class="comment">#如何取出第6-8张（包括6不包括8）图片中中心区域（14*14）的部分</span></span><br><span class="line">e=a[<span class="number">5</span>:<span class="number">7</span>,<span class="number">7</span>:<span class="number">21</span>,<span class="number">7</span>:<span class="number">21</span>,:]</span><br><span class="line"><span class="comment">#如何将图片根据通道拆分成三份单通道图片？</span></span><br><span class="line">f1,f2,f3=tf.split(a, [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], <span class="number">3</span>)</span><br><span class="line"><span class="comment">#写出`tf.shape(img)`返回的张量的阶数以及`shape`属性的值。</span></span><br><span class="line">g=tf.shape(a) </span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment">#print(sess.run(a))</span></span><br><span class="line">    <span class="comment">#print(sess.run(b))</span></span><br><span class="line">    print(sess.run(g))</span><br><span class="line">    print(sess.run(tf.rank(g)))</span><br></pre></td></tr></table></figure>
<h1 id="4-常量-变量-占位符"><a href="#4-常量-变量-占位符" class="headerlink" title="4.常量,变量,占位符"></a>4.常量,变量,占位符</h1><h3 id="常量"><a href="#常量" class="headerlink" title="常量"></a>常量</h3><p>常量是一块只读的内存区域，常量在<strong>初始化时就必须赋值</strong>，并且之后值将不能被改变。Python并无内置常量关键字，需要用到时往往需要我们去实现，而Tensorflow内置了常量方法 <code>tf.constant()</code>。</p>
<h3 id="普通常量"><a href="#普通常量" class="headerlink" title="普通常量"></a>普通常量</h3><p>普通常量使用<code>tf.constant()</code>初始化得到，其有5个参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">constant(</span><br><span class="line">    value, </span><br><span class="line">    dtype=<span class="keyword">None</span>, </span><br><span class="line">    shape=<span class="keyword">None</span>, </span><br><span class="line">    name=<span class="string">"Const"</span>, </span><br><span class="line">    verify_shape=<span class="keyword">False</span>):</span><br></pre></td></tr></table></figure>
<ul>
<li><code>value</code>是必填参数，即常量的初识值。这里需要注意，这个<code>value</code>可以是Python中的<code>list</code>、<code>tuple</code>以及<code>Numpy</code>中的<code>ndarray</code>对象，但<strong>不可以是Tensor对象</strong>，因为这样没有意义。</li>
<li><code>dtype</code>可选参数，表示数据类型，<code>value</code>中的数据类型应与与<code>dtype</code>中的类型一致，如果不填写则会根据<code>value</code>中值的类型进行推断。</li>
<li><code>shape</code> 可选参数，表示value的形状。如果参数<code>verify_shape=False</code>，<code>shape</code>在与<code>value</code>形状不一致时会修改<code>value</code>的形状。如果参数<code>verify_shape=True</code>，则要求<code>shape</code>必须与<code>value</code>的<code>shape</code>一致。当<code>shape</code>不填写时，默认为<code>value</code>的<code>shape</code>。</li>
</ul>
<p><strong>注意</strong>：<code>tf.constant()</code>生成的是一个张量。其类型是<code>tf.Tensor</code>。</p>
<p><strong>常量存储在图的定义当中</strong>，可以将图序列化后进行查看：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">const_a = tf.constant([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># tf.Graph.as_graph_def 返回一个代表当前图的序列化的`GraphDef`</span></span><br><span class="line">    print(sess.graph.as_graph_def()) <span class="comment"># 你将能够看到const_a的值</span></span><br></pre></td></tr></table></figure>
<h3 id="序列常量"><a href="#序列常量" class="headerlink" title="序列常量"></a>序列常量</h3><p>除了使用<code>tf.constant()</code>生成任意常量以外，我们还可以使用一些方法快捷的生成<strong>序列常量</strong>：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在指定区间内生成均匀间隔的数字</span></span><br><span class="line">tf.linspace(start,stop,num,name=<span class="keyword">None</span>)</span><br><span class="line">tf.linspace(10.0,13.0,4) ==&gt; [10.0 11.0 12.0 13.0] </span><br><span class="line"><span class="comment"># 在指定区间内生成均匀间隔的数字 类似于python中的range</span></span><br><span class="line">tf.range(start, limit=<span class="keyword">None</span>, delta=<span class="number">1</span>, dtype=<span class="keyword">None</span>, name=<span class="string">'range'</span>)</span><br><span class="line">tf.range(start=3, limit=18, delta=3)  ==&gt; [3, 6, 9, 12, 15]</span><br><span class="line">tf.range(5) ==&gt; [0, 1, 2, 3, 4]</span><br></pre></td></tr></table></figure>
<h3 id="随机数常量"><a href="#随机数常量" class="headerlink" title="随机数常量"></a>随机数常量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成服从正态分布的随机数</span></span><br><span class="line">tf.random_normal(shape, mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>, dtype=tf.float32, seed=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成服从截断的正态分布的随机数</span></span><br><span class="line"><span class="comment"># 只保留了两个标准差以内的值，超出的值会被丢掉重新生成</span></span><br><span class="line">tf.truncated_normal(shape, mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>, dtype=tf.float32, seed=<span class="keyword">None</span>,</span><br><span class="line">name=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成服从均匀分布的随机值</span></span><br><span class="line">tf.random_uniform(shape, minval=<span class="number">0</span>, maxval=<span class="keyword">None</span>, dtype=tf.float32, seed=<span class="keyword">None</span>,</span><br><span class="line">name=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将输入张量在第一个维度上进行随机打乱</span></span><br><span class="line">tf.random_shuffle(value, seed=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机的将张量收缩到给定的尺寸</span></span><br><span class="line"><span class="comment"># 注意：不是打乱，是随机的在某个位置开始裁剪指定大小的样本</span></span><br><span class="line"><span class="comment"># 可以利用样本生成子样本</span></span><br><span class="line">tf.random_crop(value, size, seed=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<h3 id="随机数种子"><a href="#随机数种子" class="headerlink" title="随机数种子"></a>随机数种子</h3><p>随机数常量的生成依赖于两个随机数种子，一个是图级别的种子，另一个是操作级别的种子。上述所有操作当中，每个操作均可以接收一个<code>seed</code>参数，这个参数可以是任意一个整数，即为操作级种子。图级种子使用<code>tf.set_random_seed()</code>进行设置。</p>
<p><strong>注意</strong>：每个随机数种子可以确定一个随机数序列，而不是一个随机数。</p>
<p>设置随机数种子，可以使得图或其中的一部分操作在不同的会话中出现一样的随机数。具体来讲就是如果设置了某一个操作的随机数种子，则在不同的会话中，这个操作生成的随机数序列是完全一样的；如果设置了图级别的随机数种子，则这个图在不同的会话中所有生成的随机数序列都是完全一样的。</p>
<p><strong>注意</strong>：如果既设置了图级种子也设置了部分或全部随机数生成操作的种子，那么也会在不同会话中表现一样，只不过最终随机数的种子与默认的不同，其取决于二者。</p>
<p>当不设置随机数种子时，会话与会话之间的随机数生成没有关系，如下代码打印出<code>a</code>在不同会话中的结果不同：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Graph().as_default():</span><br><span class="line">    a = tf.random_uniform([])</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Session 1"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess1:</span><br><span class="line">        print(sess1.run(a))  </span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Session 2"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess2:</span><br><span class="line">        print(sess2.run(a))</span><br></pre></td></tr></table></figure>
<p>当设置了随机数种子时，两个会话中生成的随机数序列是完全一样的，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Graph().as_default():</span><br><span class="line">    a = tf.random_uniform([], seed=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess1:</span><br><span class="line">        res1 = sess1.run(a)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess2:</span><br><span class="line">        res2 = sess2.run(a)</span><br><span class="line">    </span><br><span class="line">    print(res1 == res2)  <span class="comment"># &gt;&gt;&gt; True</span></span><br></pre></td></tr></table></figure>
<p>使用图级随机数种子，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Graph().as_default():</span><br><span class="line">    tf.set_random_seed(<span class="number">1</span>)</span><br><span class="line">    a = tf.random_uniform([])</span><br><span class="line">    b = tf.random_normal([])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess1:</span><br><span class="line">        res1_a, res1_b = sess1.run([a, b])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess2:</span><br><span class="line">        res2_a, res2_b = sess2.run([a, b])</span><br><span class="line">    </span><br><span class="line">    print(res1_a == res2_a)  <span class="comment"># &gt;&gt;&gt; True</span></span><br><span class="line">    print(res1_b == res2_b)  <span class="comment"># &gt;&gt;&gt; True</span></span><br></pre></td></tr></table></figure>
<h3 id="特殊常量"><a href="#特殊常量" class="headerlink" title="特殊常量"></a>特殊常量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成指定shape的全0张量</span></span><br><span class="line">tf.zeros(shape, dtype=tf.float32, name=<span class="keyword">None</span>)</span><br><span class="line"><span class="comment"># 生成与输入的tensor相同shape的全0张量</span></span><br><span class="line">tf.zeros_like(tensor, dtype=<span class="keyword">None</span>, name=<span class="keyword">None</span>,optimize=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 生成指定shape的全1张量</span></span><br><span class="line">tf.ones(shape, dtype=tf.float32, name=<span class="keyword">None</span>)</span><br><span class="line"><span class="comment"># 生成与输入的tensor相同shap的全1张量</span></span><br><span class="line">tf.ones_like(tensor, dtype=<span class="keyword">None</span>, name=<span class="keyword">None</span>, optimize=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 生成一个使用value填充的shape是dims的张量</span></span><br><span class="line">tf.fill(dims, value, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<h3 id="变量-1"><a href="#变量-1" class="headerlink" title="变量"></a>变量</h3><p>变量用于存取张量，在Tensorflow中主要使用类<code>tf.Variable()</code>来实例化一个变量对象，作用类似于Python中的变量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tf.Variable(</span><br><span class="line">    initial_value=<span class="keyword">None</span>, </span><br><span class="line">    trainable=<span class="keyword">True</span>, </span><br><span class="line">    collections=<span class="keyword">None</span>, </span><br><span class="line">    validate_shape=<span class="keyword">True</span>, </span><br><span class="line">    caching_device=<span class="keyword">None</span>, </span><br><span class="line">    name=<span class="keyword">None</span>, </span><br><span class="line">    variable_def=<span class="keyword">None</span>, </span><br><span class="line">    dtype=<span class="keyword">None</span>, </span><br><span class="line">    expected_shape=<span class="keyword">None</span>, </span><br><span class="line">    import_scope=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p><code>initial_value</code>是必填参数，即变量的初始值。可以使用Python中的<code>list</code>、<code>tuple</code>、Numpy中的<code>ndarray</code>、<code>Tensor</code>对象或者其他变量进行初始化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用list初始化</span></span><br><span class="line">var1 = tf.Variable([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用ndarray初始化</span></span><br><span class="line">var2 = tf.Variable(np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用Tensor初始化</span></span><br><span class="line">var3 = tf.Variable(tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用服从正态分布的随机数Tensor初始化</span></span><br><span class="line">var4 = tf.Variable(tf.random_normal([<span class="number">3</span>, ]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用变量var1初始化</span></span><br><span class="line">var5 = tf.Variable(var1)</span><br></pre></td></tr></table></figure>
<p>这里需要注意的是：使用<code>tf.Variable()</code>得到的对象<strong>不是Tensor对象</strong>，而是承载了<code>Tensor</code>对象的<strong><code>Variable</code>对象</strong>。<code>Tensor</code>对象就行是一个“流动对象”，可以存在于各种操作中，包括存在于<code>Variable</code>中。所以这也涉及到了如何给变量对象赋值、取值问题。</p>
<h3 id="使用tf-get-variable-创建变量"><a href="#使用tf-get-variable-创建变量" class="headerlink" title="使用tf.get_variable()创建变量"></a>使用<code>tf.get_variable()</code>创建变量</h3><p>除了使用<code>tf.Variable()</code>类实例化一个变量对象以外，还有一种常用的方法来产生一个变量对象：<code>tf.get_variable()</code>这是一个生成或获取变量对象的函数。需要注意的是，使用<code>tf.get_variable()</code>方法生成一个变量时，其<strong>name不能与已有的name重名</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成一个shape为[3, ]的变量，变量的初值是随机的。</span></span><br><span class="line">tf.get_variable(name=<span class="string">'get_var'</span>, shape=[<span class="number">3</span>, ])</span><br><span class="line"><span class="comment"># &lt;tf.Variable 'get_var:0' shape=(3,) dtype=float32_ref&gt;</span></span><br></pre></td></tr></table></figure>
<p>一般的，<code>tf.get_variable()</code>与<code>variable_scope</code>配合使用会非常方便</p>
<h3 id="变量初始化"><a href="#变量初始化" class="headerlink" title="变量初始化"></a>变量初始化</h3><p>变量作为操作的一种，是可以参与图的构建与运行的，但变量在会话中运行时必须在之前进行初始化操作。通常的变量初始化在会话中较早初始化，这可以使之后变量可以正常运行。变量的初始化也是一个操作</p>
<p>1.使用变量的属性<code>initializer</code>进行初始化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">var = tf.Variable([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 如果没有初始化操作，则抛出异常</span></span><br><span class="line">    sess.run(var.initializer)</span><br><span class="line">    print(sess.run(var))  <span class="comment"># &gt;&gt;&gt; [1, 2, tf.variables_initializer([var1, var2]3]tf.variables_initializer([var1, var2]</span></span><br></pre></td></tr></table></figure>
<p>2.单独初始化每一个变量较为繁琐，可以使用<code>tf.variables_initializer()</code>初始化一批变量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">var1 = tf.Variable([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">var2 = tf.Variable([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.variables_initializer([var1,var2])</span><br></pre></td></tr></table></figure>
<p>3.上述方法仍然较为繁琐，一般的，所有的变量均需要初始化，这时候就不再需要特别申明。直接使用<code>tf.global_variables_initialize()</code>与<code>tf.local_variables_initializer()</code>即可初始化全部变量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">var1 = tf.Variable(tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=tf.float32))</span><br><span class="line">var2 = tf.Variable(tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="comment"># 此处并不存在局部变量，所以不需要`tf.local_variables_initializer()`初始化也可以</span></span><br><span class="line">    sess.run(tf.local_variables_initializer())</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">var1 = tf.Variable([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">tensor1 = var1.initialized_value()</span><br></pre></td></tr></table></figure>
<h3 id="变量赋值"><a href="#变量赋值" class="headerlink" title="变量赋值"></a>变量赋值</h3><p>变量赋值包含两种情况，第一种情况是定义时进行赋值，第二种是在图运行时修改变量的值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义时赋予变量一个值</span></span><br><span class="line">A = tf.Variable([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])  </span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(sess.run(A))  <span class="comment"># &gt;&gt; [1, 2, 3]</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 赋值方法一</span></span><br><span class="line">    sess.run(tf.assign(A, [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]))  </span><br><span class="line">    print(sess.run(A))  <span class="comment"># &gt;&gt; [2, 3, 4]</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 赋值方法二</span></span><br><span class="line">    sess.run(A.assign([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]))</span><br><span class="line">    print(sess.run(A))  <span class="comment"># &gt;&gt; [2, 3, 4]</span></span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>：使用<code>tf.Variable.assign()</code>或<code>tf.assign()</code>进行赋值时，必须要求所赋的值的<code>shape</code>与<code>Variable</code>对象中张量的<code>shape</code>一样、<code>dtype</code>一样。</p>
<p>除了使用<code>tf.assign()</code>以外还可以使用<code>tf.assign_add()</code>、<code>tf.assign_sub()</code>进行增量赋值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">A = tf.Variable(tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]))</span><br><span class="line"></span><br><span class="line">a=tf.assign_add(A, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>]) </span><br><span class="line">b=tf.assign_sub(A, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>])  </span><br><span class="line">init=tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    </span><br><span class="line">    print(sess.run(a))  <span class="comment">#[2 3 6]</span></span><br><span class="line">    print(sess.run(b))  <span class="comment">#[1 2 3]</span></span><br></pre></td></tr></table></figure>
<h3 id="变量操作注意事项"><a href="#变量操作注意事项" class="headerlink" title="变量操作注意事项"></a>变量操作注意事项</h3><ul>
<li><p>当我们在会话中运行并输出一个初始化并再次复制的变量时，输出是多少？如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">W=tf.Variable(<span class="number">10</span>)</span><br><span class="line">W.assign(<span class="number">100</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(W.initializer)</span><br><span class="line">    sess.run(W)  <span class="comment">#10</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>重复运行变量赋值语句会发生什么？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">var = tf.Variable(<span class="number">1</span>)</span><br><span class="line">assign_op = var.assign(<span class="number">2</span> * var)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(var.initializer)</span><br><span class="line">    sess.run(assign_op)</span><br><span class="line">    sess.run(var)  <span class="comment"># &gt; 2</span></span><br><span class="line">    </span><br><span class="line">    sess.run(assign_op)  </span><br><span class="line">    sess.run(var)  <span class="comment"># &gt; 4</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">var = tf.Variable(<span class="number">1</span>)</span><br><span class="line">assign_op = var.assign(<span class="number">2</span> * var)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(var.initializer)</span><br><span class="line">    sess.run([assign_op, assign_op])  </span><br><span class="line">    sess.run(var)  <span class="comment"># 2</span></span><br><span class="line">    <span class="comment">#这里，会输出`2`。会话`run`一次，图执行一次，而`sess.run([assign_op, assign_op])`仅仅相当于查看了两次执行结果，并不是执行了两次。</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">var = tf.Variable(<span class="number">1</span>)</span><br><span class="line">assign_op_1 = var.assign(<span class="number">2</span> * var)</span><br><span class="line">assign_op_2 = var.assign(<span class="number">3</span> * var)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(var.initializer)</span><br><span class="line">    sess.run([assign_op_1, assign_op_2])</span><br><span class="line">    sess.run(var)</span><br></pre></td></tr></table></figure>
<p>这里两次赋值的Op相当于一个图中的两个子图，其执行顺序不分先后，由于两个子图的执行结果会对公共的变量产生影响，当子图A的执行速度快于子图B时，可能是一种结果，反之是另一种结果，所以这样的写法是不安全的写法，执行的结果是不可预知的，所以要避免此种情况出现。但可以通过<strong>控制依赖</strong>来强制控制两个子图的执行顺序。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">var = tf.Variable(<span class="number">1</span>)</span><br><span class="line">assign_op_1 = var.assign(<span class="number">2</span> * var)</span><br><span class="line"></span><br><span class="line">g=tf.get_default_graph()</span><br><span class="line"><span class="keyword">with</span> g.control_dependencies([assign_op_1]):</span><br><span class="line">    assign_op_2 = var.assign(<span class="number">3</span> * var)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(var.initializer)</span><br><span class="line">    sess.run([assign_op_1, assign_op_2])</span><br><span class="line">    print(sess.run(var)) <span class="comment">#6</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>在多个图中给一个变量赋值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">W = tf.Variable(<span class="number">10</span>)</span><br><span class="line">sess1 = tf.Session()</span><br><span class="line">sess2 = tf.Session()</span><br><span class="line"></span><br><span class="line">sess1.run(W.initializer)</span><br><span class="line">sess2.run(W.initializer)</span><br><span class="line"></span><br><span class="line">print(sess1.run(W.assign_add(<span class="number">10</span>)))  <span class="comment"># &gt;&gt; 20</span></span><br><span class="line">print(sess2.run(W.assign_sub(<span class="number">2</span>)))  <span class="comment"># 8</span></span><br><span class="line"></span><br><span class="line">sess1.close()</span><br><span class="line">sess2.close()</span><br><span class="line"><span class="comment">#因为在两个图中的OP是互不相干的。**每个会话都保留自己的变量副本**，它们分别执行得到结果。</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>使用一个变量初始化另一个变量时：</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = tf.Variable(<span class="number">1</span>)</span><br><span class="line">b = tf.Variable(a)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(b.initializer)  <span class="comment"># 抛出异常</span></span><br></pre></td></tr></table></figure>
<p>出错的原因是<code>a</code>没有初始化，<code>b</code>就无法初始化。所以使用一个变量初始化另一个变量时，会带来不安全因素。为了确保初始化时不会出错，可以使用如下方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = tf.Variable(<span class="number">1</span>)</span><br><span class="line">b = tf.Variable(a.initialized_value())</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(b.initializer)</span><br><span class="line">    print(sess.run(b))</span><br></pre></td></tr></table></figure>
<ul>
<li><p>变量初始化操作应该置于其他变量操作之前。事实上所有在一次会话执行中的操作与张量，都应该考虑其执行顺序是否会有关联，如有关联，则可能出现不可预知的错误。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">W = tf.Variable(<span class="number">10</span>)</span><br><span class="line">assign_op = W.assign(<span class="number">100</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run([W.initializer, assign_op])</span><br><span class="line">    print(sess.run(W))  <span class="comment"># W 可能为10或100</span></span><br></pre></td></tr></table></figure>
<p>正确的写法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">W = tf.Variable(<span class="number">10</span>)</span><br><span class="line">assign_op = W.assign(<span class="number">100</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(W.initializer)</span><br><span class="line">    sess.run(assign_op)</span><br><span class="line">    print(sess.run(W))</span><br></pre></td></tr></table></figure>
</li>
<li><p>重新初始化变量之后，变量的值变为初始值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">W = tf.Variable(<span class="number">10</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(W.initializer)</span><br><span class="line">    sess.run(W.assign(<span class="number">100</span>))</span><br><span class="line">    sess.run(W.initializer)</span><br><span class="line">    print(sess.run(W))  <span class="comment"># &gt;&gt;&gt; 10</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Tensor、ndarray、原生数据之间的相互转化"><a href="#Tensor、ndarray、原生数据之间的相互转化" class="headerlink" title="Tensor、ndarray、原生数据之间的相互转化"></a>Tensor、ndarray、原生数据之间的相互转化</h3><p>一般的，构建常量、变量等图结构时经常会使用Python原生数据与数据结构作为输入，例如构建一个常量：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.constant([1, 2])</span><br></pre></td></tr></table></figure>
<p>这时等价于将Python环境中的数据转化为了TensorFlow中的张量，默认的，整形转化为<code>DT_INT32</code>类型，浮点型转化为<code>DT_FLOAT</code>（32位）类型，复数转化为<code>DT_COMPLEX128</code>类型。</p>
<h4 id="Tensor-转化为原生数据、ndarray"><a href="#Tensor-转化为原生数据、ndarray" class="headerlink" title="Tensor 转化为原生数据、ndarray"></a>Tensor 转化为原生数据、ndarray</h4><p>当完成图的构建之后，在会话中运行图则可以得到结果，这时候会话返回给Python环境中的结果也是包可能是Python原生数据或ndarray数据。一般的张量执行的结果均返回ndarray数据（这是因为TensorFlow内核使用了ndarray数据结构）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    res = sess.run(a)</span><br><span class="line">    print(res)   <span class="comment"># &gt;&gt;&gt; [1 2 3]</span></span><br><span class="line">    print(type(res))  <span class="comment"># &gt;&gt;&gt; &lt;class 'numpy.ndarray'&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="占位符"><a href="#占位符" class="headerlink" title="占位符"></a>占位符</h3><p>引入占位符在TensorFlow中是很重要的，它可以把图的构建与数据的输入关系解耦，这意味着构建一个图只需要知道数据的格式即可，而不需要将庞大的数据输入图.在TensorFlow中使用<code>tf.placeholder</code>构建占位符。占位符也是一个节点。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.placeholder(dtype, shape=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>例如，我们需要将上述方程使用带占位符的图描述：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x=tf.placeholder(dtype=tf.float32,shape=[])</span><br><span class="line">y=tf.p;aceholder(dtype=tf.float32,shape=[])</span><br><span class="line"></span><br><span class="line">z=tf.multiply(x,<span class="number">2</span>)+y</span><br></pre></td></tr></table></figure>
<h4 id="feed-dict"><a href="#feed-dict" class="headerlink" title="feed_dict"></a>feed_dict</h4><p>图构建好之后，运行图时，需要使用张量替代占位符，否则这个图无法运行，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(dtype=tf.float32, shape=[])</span><br><span class="line">y = tf.placeholder(dtype=tf.float32, shape=[])</span><br><span class="line"></span><br><span class="line">z = tf.multiply(x, <span class="number">2</span>) + y</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(z, feed_dict=&#123;x: <span class="number">5</span>, y: <span class="number">10</span>&#125;)  <span class="comment"># &gt;&gt;&gt; 20.0</span></span><br></pre></td></tr></table></figure>
<p>可以看到使用占位符，需要输入占位数据的类型即可，即不需要填入具体数据。占位符可以不设确定的<code>shape</code>意味着可以使用不同<code>shape</code>但运算规则一致的数据。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(dtype=tf.float32)</span><br><span class="line">y = tf.placeholder(dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">z = tf.multiply(x, <span class="number">2</span>) + y</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(z, feed_dict=&#123;x: <span class="number">5</span>, y: <span class="number">10</span>&#125;)  <span class="comment"># &gt;&gt;&gt; 20.0</span></span><br><span class="line">    sess.run(z, feed_dict=&#123;x: [<span class="number">5</span>, <span class="number">4</span>], y: [<span class="number">10</span>, <span class="number">18</span>]&#125;)  <span class="comment"># &gt;&gt;&gt; [20., 26.]</span></span><br></pre></td></tr></table></figure>
<h4 id="feed-dict的更多用法"><a href="#feed-dict的更多用法" class="headerlink" title="feed_dict的更多用法"></a>feed_dict的更多用法</h4><p>除了<code>tf.placeholder</code>可以并且必须使用张量替代以外，很多张量均可以使用<code>feed_dict</code>替代，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = multiply(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(a, feed_dict=&#123;a:<span class="number">10</span>&#125;)  <span class="comment"># &gt;&gt; 10</span></span><br></pre></td></tr></table></figure>
<p>为了保证张量的替代是合法的，可以使用<code>tf.Graph.is_feedable(tensor)</code>检查<code>tensor</code>是否可以替代：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = tf.multiply(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(a, feed_dict=&#123;a:<span class="number">10</span>&#125;))  <span class="comment"># &gt;&gt; 10</span></span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>：变量、占位符是两类完全不同功能的节点。变量在图的运行中负责保存当前时刻一个张量的具体数值，在运行的不同阶段、时刻可能是不同的，职责是存储运行时的临时张量；而占位符是在图运行之前的充当某一个张量的替身，在运行时必须使用一个张量替代，职责是建构时代替具体的张量值。</p>
<h1 id="5-名字与作用域"><a href="#5-名字与作用域" class="headerlink" title="5.名字与作用域"></a>5.名字与作用域</h1><p>Tensorflow的作用域分为两种，一种是<code>variable_scope</code>，另一种是<code>name_scope</code>。</p>
<h3 id="name"><a href="#name" class="headerlink" title="name"></a>name</h3><p><code>Tensor</code>与<code>Operation</code>均有<code>name</code>属性，但我们只能给<code>Operation</code>进行主动命名，<code>Tensor</code>的<code>name</code>由<code>Operation</code>根据自己的<code>name</code>与输出数量进行命名（所有的<code>Tensor</code>均由<code>Operation</code>产生）。</p>
<p>例如，我们定义一个常量，并赋予其<code>name</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], name=<span class="string">'const'</span>)</span><br></pre></td></tr></table></figure>
<p>这里我们给常量<code>Op</code>定义了一个<code>name</code>为<code>const</code>。<code>a</code>是常量<code>Op</code>的返回值（输出），是一个张量<code>Tensor</code>对象，所以<code>a</code>也有自己的<code>name</code>，为<code>const:0</code>。</p>
<h4 id="Op的name命名规范"><a href="#Op的name命名规范" class="headerlink" title="Op的name命名规范"></a>Op的name命名规范</h4><p>首先<code>Tensor</code>对象的<code>name</code>我们并不能直接进行操作，我们只能给<code>Op</code>设置<code>name</code>。<code>Op</code>的命令规范规范是：<strong>由数字、字母、下划线组成，不能以下划线开头，且不区分大小写</strong>。</p>
<p><strong>正确</strong>的命名方式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a1 = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], name=<span class="string">'const'</span>)</span><br><span class="line">a2 = tf.Variable([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], name=<span class="string">'123'</span>)</span><br><span class="line">a3 = tf.add(<span class="number">1</span>, <span class="number">2</span>, name=<span class="string">'const_'</span>)</span><br></pre></td></tr></table></figure>
<p><strong>错误</strong>的命名方式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a1 = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], name=<span class="string">'_const'</span>)</span><br><span class="line">a2 = tf.Variable([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], name=<span class="string">'/123'</span>)</span><br><span class="line">a3 = tf.add(<span class="number">1</span>, <span class="number">2</span>, name=<span class="string">'const:0'</span>)</span><br></pre></td></tr></table></figure>
<p>每个<code>Op</code>都有<code>name</code>属性，可以通过属性查看<code>name</code>值，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 返回一个什么都不做的Op</span></span><br><span class="line">op = tf.no_op(name=<span class="string">'hello'</span>)</span><br><span class="line"></span><br><span class="line">print(op.name)  <span class="comment"># hello</span></span><br></pre></td></tr></table></figure>
<p>这里我们列举了一个空<code>Op</code>，而没有使用常用的诸如<code>tf.add</code>这样的<code>Op</code>，是因为默认的大部分<code>Op</code>都返回对应的张量，而不是<code>Op</code>对象，<strong>但<code>tf.no_op</code>函数返回的是<code>Op</code>对象</strong>，是一个特例。</p>
<h4 id="Tensor的name构成"><a href="#Tensor的name构成" class="headerlink" title="Tensor的name构成"></a>Tensor的name构成</h4><p>大部分的<code>Op</code>会有返回值，其返回值一般是一个或多个<code>Tensor</code>。<code>Tensor</code>的<code>name</code>并不来源于我们设置，只能来源于生成它的<code>Op</code>，所以<code>Tensor</code>的<code>name</code>是由<code>Op</code>的<code>name</code>所决定的。</p>
<p><code>Tensor</code>的<code>name</code>构成很简单，即在对应的<code>Op</code>的<code>name</code>之后加上输出索引。即由以下三部分构成：</p>
<ol>
<li>生成此<code>Tensor</code>的<code>op</code>的<code>name</code>；</li>
<li>冒号；</li>
<li><code>op</code>输出内容的索引，索引默认从<code>0</code>开始。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], name=<span class="string">'const'</span>)</span><br><span class="line"></span><br><span class="line">print(a.name)  <span class="comment"># const:0</span></span><br></pre></td></tr></table></figure>
<p>这里，我们设置了常量<code>Op</code>的<code>name</code>为<code>const</code>，这个<code>Op</code>会返回一个<code>Tensor</code>，所以返回的<code>Tensor</code>的<code>name</code>就是在其后加上冒号与索引。由于只有一个输出，所以这个输出的索引就是<code>0</code>。</p>
<p>对于两个或多个的输出，其索引依次增加：如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">key, value = tf.ReaderBase.read(..., name=<span class="string">'read'</span>)</span><br><span class="line"></span><br><span class="line">print(key.name)  <span class="comment"># read:0</span></span><br><span class="line">print(value.name)  <span class="comment"># read:1</span></span><br></pre></td></tr></table></figure>
<h4 id="Op与Tensor的默认name"><a href="#Op与Tensor的默认name" class="headerlink" title="Op与Tensor的默认name"></a>Op与Tensor的默认name</h4><p>当我们不去设置<code>Op</code>的<code>name</code>时，TensorFlow也会默认设置一个<code>name</code>，这也正是<code>name</code>为可选参数的原因。默认<code>name</code>往往与<code>Op</code>的类型相同（默认的<code>name</code>并无严格的命名规律）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">a = tf.add(<span class="number">1</span>, <span class="number">2</span>)  </span><br><span class="line"><span class="comment"># op name为 `Add`</span></span><br><span class="line"><span class="comment"># Tensor name 为 `Add:0`</span></span><br><span class="line"></span><br><span class="line">b = tf.constant(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># op name 为 `Const`</span></span><br><span class="line"><span class="comment"># Tensor name 为 `Const:0`</span></span><br><span class="line"></span><br><span class="line">c = tf.divide(tf.constant(<span class="number">1</span>), tf.constant(<span class="number">2</span>))</span><br><span class="line"><span class="comment"># op name 为 `truediv`</span></span><br><span class="line"><span class="comment"># Tensor name 为 `truediv:0`</span></span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>：还有一些特殊的<code>Op</code>，我们没法指定其<code>name</code>，只能使用默认的<code>name</code>，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">print(init.name)  <span class="comment"># init</span></span><br></pre></td></tr></table></figure>
<h4 id="重复name的处理方式"><a href="#重复name的处理方式" class="headerlink" title="重复name的处理方式"></a>重复name的处理方式</h4><p>虽然<code>name</code>作为唯一性的标识符，但TensorFlow并不会强制要求我们必须设置完全不同的<code>name</code>，这并非说明<code>name</code>可以重复，而是TensorFlow通过一些方法避免了<code>name</code>重复。</p>
<p>当出现了两个<code>Op</code>设置相同的<code>name</code>时，TensorFlow会自动给后面的<code>op</code>的<code>name</code>加一个后缀。如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a1 = tf.add(<span class="number">1</span>, <span class="number">2</span>, name=<span class="string">'my_add'</span>)</span><br><span class="line">a2 = tf.add(<span class="number">3</span>, <span class="number">4</span>, name=<span class="string">'my_add'</span>)</span><br><span class="line"></span><br><span class="line">print(a1.name)  <span class="comment"># my_add:0</span></span><br><span class="line">print(a2.name)  <span class="comment"># my_add_1:0</span></span><br></pre></td></tr></table></figure>
<p>后缀由下划线与索引组成（注意与<code>Tensor</code>的<code>name</code>属性后的缀冒号索引区分）。从重复的第二个<code>name</code>开始加后缀，<strong>后缀的索引从<code>1</code>开始</strong>。</p>
<p>当我们不指定<code>name</code>时，使用默认的<code>name</code>也是相同的处理方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a1 = tf.add(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">a2 = tf.add(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">print(a1.name)  <span class="comment"># Add:0</span></span><br><span class="line">print(a2.name)  <span class="comment"># Add_1:0</span></span><br></pre></td></tr></table></figure>
<p>不同操作之间是有相同的<code>name</code>也是如此：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a1 = tf.add(<span class="number">1</span>, <span class="number">2</span>, name=<span class="string">'my_name'</span>)</span><br><span class="line">a2 = tf.subtract(<span class="number">1</span>, <span class="number">2</span>, name=<span class="string">'my_name'</span>)</span><br><span class="line"></span><br><span class="line">print(a1.name)  <span class="comment"># &gt;&gt;&gt; my_name:0</span></span><br><span class="line">print(a2.name)  <span class="comment"># &gt;&gt;&gt; my_name_1:0</span></span><br></pre></td></tr></table></figure>
<h3 id="不同图中相同操作name"><a href="#不同图中相同操作name" class="headerlink" title="不同图中相同操作name"></a>不同图中相同操作name</h3><p>当我们构建了两个或多个图的时候，如果这些图中有相同的操作或者相同的<code>name</code>时，并不会互相影响。如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">g1 = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g1.as_default():</span><br><span class="line">    a1 = tf.add(<span class="number">1</span>, <span class="number">2</span>, name=<span class="string">'add'</span>)</span><br><span class="line">    print(a1.name)  <span class="comment"># add:0</span></span><br><span class="line"></span><br><span class="line">g2 = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g2.as_default():</span><br><span class="line">    a2 = tf.add(<span class="number">1</span>, <span class="number">2</span>, name=<span class="string">'add'</span>)</span><br><span class="line">    print(a2.name)  <span class="comment"># add:0</span></span><br></pre></td></tr></table></figure>
<p><strong>小练习</strong>：</p>
<p>以下操作均为一个图中的<code>op</code>，请写出以下操作对应中的<code>Op</code>与对应生成的<code>Tensor</code>的<code>name</code>：</p>
<ul>
<li><code>tf.constant([1, 2])</code>  #const:0</li>
<li><code>tf.add([1, 2], [3, 4], name=&#39;op_1&#39;)</code> #op_1:0</li>
<li><code>tf.add([2, 3], [4, 5], name=&#39;op_1&#39;)</code> #op_1_1:0</li>
<li><code>tf.mod([1, 3], [2, 4], name=&#39;op_1&#39;)</code> #op_1_2:0</li>
<li><code>tf.slice([1, 2], [0], [1], name=&#39;123&#39;)</code># 123:0 </li>
</ul>
<h3 id="通过name获取Op与Tensor"><a href="#通过name获取Op与Tensor" class="headerlink" title="通过name获取Op与Tensor"></a>通过name获取Op与Tensor</h3><p>上文，我们介绍了<code>name</code>可以看做是<code>Op</code>与<code>Tensor</code>的唯一标识符。所以可以通过一些方法利用<code>name</code>获取到<code>Op</code>与<code>Tensor</code>。</p>
<p>例如，一个计算过程如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">g1 = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g1.as_default():</span><br><span class="line">	a = tf.add(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">	b = tf.multiply(a, <span class="number">10</span>)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=g1) <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(b)  <span class="comment"># 80</span></span><br></pre></td></tr></table></figure>
<p>上述图，我们使用了Python变量<code>b</code>获取对应的操作，我们也可以使用如下方式获取，两种方式结果一样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">g1 = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g1.as_default():</span><br><span class="line">    tf.add(<span class="number">3</span>, <span class="number">5</span>, name=<span class="string">'add'</span>)</span><br><span class="line">    tf.multiply(g1.get_tensor_by_name(<span class="string">'add:0'</span>), <span class="number">10</span>, name=<span class="string">'mul'</span>)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=g1) <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(g1.get_tensor_by_name(<span class="string">'mul:0'</span>))  <span class="comment"># 80</span></span><br></pre></td></tr></table></figure>
<p>这里使用了<code>tf.Graph.get_tensor_by_name</code>方法。可以根据<code>name</code>获取<code>Tensor</code>。其返回值是一个<code>Tensor</code>对象。这里要注意<code>Tensor</code>的<code>name</code>必须写完整，请勿将<code>Op</code>的<code>name</code>当做是<code>Tensor</code>的<code>name</code>。</p>
<p>同样的，利用<code>name</code>也可以获取到相应的<code>Op</code>，这里需要使用<code>tf.Graph.get_operation_by_name</code>方法。上述例子中，我们在会话中运行的是乘法操作的返回值<code>b</code>。运行<code>b</code>的时候，与其相关的依赖，包括乘法<code>Op</code>也运行了，当我们不需要返回值时，我们在会话中可以直接运行<code>Op</code>，而无需运行<code>Tensor</code>。</p>
<p>例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">g1 = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g1.as_default():</span><br><span class="line">    tf.add(<span class="number">3</span>, <span class="number">5</span>, name=<span class="string">'add'</span>)</span><br><span class="line">    tf.multiply(g1.get_tensor_by_name(<span class="string">'add:0'</span>), <span class="number">10</span>, name=<span class="string">'mul'</span>)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=g1) <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(g1.get_operation_by_name(<span class="string">'mul'</span>))  <span class="comment"># None</span></span><br></pre></td></tr></table></figure>
<h3 id="name-scope"><a href="#name-scope" class="headerlink" title="name_scope"></a>name_scope</h3><p>随着构建的图越来越复杂，直接使用<code>name</code>对图中的节点命名会出现一些问题。比如功能近似的节点<code>name</code>可能命名重复，也难以通过<code>name</code>对不同功能的节点加以区分，这时候如果可视化图会发现将全部节点展示出来是杂乱无章的。为了解决这些问题，可以使用<code>name_scope</code>。</p>
<p><code>name_scope</code>可以为其作用域中的节点的<code>name</code>添加一个或多个前缀，并使用这些前缀作为划分内部与外部<code>op</code>范围的标记。同时在TensorBoard可视化时可以作为一个整体出现（也可以展开）。并且<code>name_scope</code>可以嵌套使用，代表不同层级的功能区域的划分。</p>
<p><code>name_scope</code>使用<code>tf.name_scope()</code>创建，返回一个上下文管理器。<code>name_scope</code>的参数<code>name</code>可以是字母、数字、下划线，不能以下划线开头。类似于<code>Op</code>的<code>name</code>的命名方式。</p>
<p><code>tf.name_scope()</code>的详情如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.name_scope(</span><br><span class="line">    name,  <span class="comment"># 传递给Op name的前缀部分</span></span><br><span class="line">    default_name=<span class="keyword">None</span>,  <span class="comment"># 默认name</span></span><br><span class="line">    values=<span class="keyword">None</span>)  <span class="comment"># 检测values中的tensor是否与下文中的Op在一个图中</span></span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>：<code>values</code>参数可以不填。当存在多个图时，可能会出现在当前图中使用了在别的图中的<code>Tensor</code>的错误写法，此时如果不在<code>Session</code>中运行图，并不会抛出异常，而填写到了<code>values</code>参数的中的<code>Tensor</code>都会检测其所在图是否为当前图，提高安全性。</p>
<p>使用<code>tf.name_scope()</code>的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant(<span class="number">1</span>, name=<span class="string">'const'</span>)</span><br><span class="line">print(a.name)  <span class="comment"># &gt;&gt; const:0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'scope_name'</span>) <span class="keyword">as</span> name:</span><br><span class="line">  	print(name)  <span class="comment"># &gt;&gt; scope_name/</span></span><br><span class="line">  	b = tf.constant(<span class="number">1</span>, name=<span class="string">'const'</span>)</span><br><span class="line">    print(b.name)  <span class="comment"># &gt;&gt; scope_name/const:0</span></span><br></pre></td></tr></table></figure>
<p>在一个<code>name_scope</code>的作用域中，可以填写<code>name</code>相同的<code>Op</code>，但TensorFlow会自动加后缀，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'scope_name'</span>) <span class="keyword">as</span> name:</span><br><span class="line">    a1 = tf.constant(<span class="number">1</span>, name=<span class="string">'const'</span>)</span><br><span class="line">    print(b.name)  <span class="comment"># scope_name/const:0</span></span><br><span class="line">    a2 = tf.constant(<span class="number">1</span>, name=<span class="string">'const'</span>)</span><br><span class="line">    print(c.name)  <span class="comment"># scope_name/const_1:0</span></span><br></pre></td></tr></table></figure>
<h3 id="多个name-scope"><a href="#多个name-scope" class="headerlink" title="多个name_scope"></a>多个name_scope</h3><p>我们可以指定任意多个<code>name_scope</code>，并且可以填写相同<code>name</code>的两个或多个<code>name_scope</code>，但TensorFlow会自动给<code>name_scope</code>的<code>name</code>加上后缀：</p>
<p>如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'my_name'</span>) <span class="keyword">as</span> name1:</span><br><span class="line">  	print(name1)  <span class="comment"># &gt;&gt; my_name/</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'my_name'</span>) <span class="keyword">as</span> name2:</span><br><span class="line">  	print(name2)  <span class="comment">#&gt;&gt; my_name_1/</span></span><br></pre></td></tr></table></figure>
<h3 id="多级name-scope"><a href="#多级name-scope" class="headerlink" title="多级name_scope"></a>多级name_scope</h3><p><code>name_scope</code>可以嵌套，嵌套之后的<code>name</code>包含上级<code>name_scope</code>的<code>name</code>。通过嵌套，可以实现多样的命名，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'name1'</span>):</span><br><span class="line">  	<span class="keyword">with</span> tf.name_scope(<span class="string">'name2'</span>) <span class="keyword">as</span> name2:</span><br><span class="line">      	print(name2)  <span class="comment"># &gt;&gt; name1/name2/</span></span><br></pre></td></tr></table></figure>
<p>不同级的<code>name_scope</code>可以填入相同的<code>name</code>（不同级的<code>name_scope</code>不存在同名），如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'name1'</span>) <span class="keyword">as</span> name1:</span><br><span class="line">    print(name1)  <span class="comment"># &gt;&gt; name1/</span></span><br><span class="line">  	<span class="keyword">with</span> tf.name_scope(<span class="string">'name1'</span>) <span class="keyword">as</span> name2:</span><br><span class="line">      	print(name2)  <span class="comment"># &gt;&gt; name1/name1/</span></span><br></pre></td></tr></table></figure>
<p>在多级<code>name_scope</code>中，<code>Op</code>的<code>name</code>会被累加各级前缀，这个前缀取决于所在的<code>name_scope</code>的层级。不同级中的<code>name</code>因为其前缀不同，所以不可能重名，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'name1'</span>):</span><br><span class="line">  	a = tf.constant(<span class="number">1</span>, name=<span class="string">'const'</span>)</span><br><span class="line">    print(a.name)  <span class="comment"># &gt;&gt; name1/const:0</span></span><br><span class="line">  	<span class="keyword">with</span> tf.name_scope(<span class="string">'name2'</span>):</span><br><span class="line">      	b = tf.constant(<span class="number">1</span>, name=<span class="string">'const'</span>)</span><br><span class="line">    	print(b.name)  <span class="comment"># &gt;&gt; name1/name2/const:0</span></span><br></pre></td></tr></table></figure>
<h4 id="name-scope的作用范围"><a href="#name-scope的作用范围" class="headerlink" title="name_scope的作用范围"></a>name_scope的作用范围</h4><p>使用<code>name_scope</code>可以给<code>Op</code>的<code>name</code>加前缀，但不包括<code>tf.get_variable()</code>创建的变量，如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'name'</span>):</span><br><span class="line">  	var = tf.Variable([<span class="number">1</span>, <span class="number">2</span>], name=<span class="string">'var'</span>)</span><br><span class="line">    print(var.name)  <span class="comment"># &gt;&gt; name/var:0</span></span><br><span class="line">    var2 = tf.get_variable(name=<span class="string">'var2'</span>, shape=[<span class="number">2</span>, ])</span><br><span class="line">    print(var2.name)  <span class="comment"># &gt;&gt; var2:0</span></span><br></pre></td></tr></table></figure>
<p><strong>这是因为<code>tf.get_variable</code>是一种特殊的操作，其只能与<code>variable_scope</code>配合完成相应功能。</strong></p>
<h4 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h4><p>从外部传入的<code>Tensor</code>，并不会在<code>name_scope</code>中加上前缀。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant(<span class="number">1</span>, name=<span class="string">'const'</span>)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'my_name'</span>, values=[a]):</span><br><span class="line">    print(a.name)  <span class="comment"># &gt;&gt; const:0</span></span><br></pre></td></tr></table></figure>
<p><code>Op</code>与<code>name_scope</code>的<code>name</code>中可以使用<code>/</code>，但<code>/</code>并不是<code>name</code>的构成，而是区分命名空间的符号，不推荐直接使用<code>/</code>。</p>
<h3 id="variable-scope"><a href="#variable-scope" class="headerlink" title="variable_scope"></a>variable_scope</h3><p><code>variable_scope</code>主要用于管理变量作用域以及与变量相关的操作，同时<code>variable_scope</code>也可以像<code>name_scope</code>一样用来给不同操作区域划分范围（添加<code>name</code>前缀）。<code>variable_scope</code>功能也要更丰富，最重要的是可以与<code>tf.get_variable()</code>等配合使用完成对变量的重复使用。</p>
<p><code>variable_scope</code>使用<code>tf.variable_scope()</code>创建，返回一个上下文管理器。</p>
<p><code>tf.variable_scope</code>的详情如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">variable_scope(name_or_scope,  <span class="comment"># 可以是name或者别的variable_scope</span></span><br><span class="line">               default_name=<span class="keyword">None</span>,</span><br><span class="line">               values=<span class="keyword">None</span>,</span><br><span class="line">               initializer=<span class="keyword">None</span>,  <span class="comment"># 作用域中的变量默认初始化方法</span></span><br><span class="line">               regularizer=<span class="keyword">None</span>,  <span class="comment"># 作用域中的变量默认正则化方法</span></span><br><span class="line">               caching_device=<span class="keyword">None</span>,  <span class="comment"># 默认缓存变量的device</span></span><br><span class="line">               partitioner=<span class="keyword">None</span>,  <span class="comment"># 用于应用在被优化之后的投影变量操作</span></span><br><span class="line">               custom_getter=<span class="keyword">None</span>,  <span class="comment"># 默认的自定义的变量getter方法</span></span><br><span class="line">               reuse=<span class="keyword">None</span>,  <span class="comment"># 变量重用状态</span></span><br><span class="line">               dtype=<span class="keyword">None</span>,  <span class="comment"># 默认的创建变量的类型</span></span><br><span class="line">               use_resource=<span class="keyword">None</span>):  <span class="comment"># 是否使用ResourceVariables代替默认的Variables</span></span><br></pre></td></tr></table></figure>
<h4 id="给Op的name加上name-scope"><a href="#给Op的name加上name-scope" class="headerlink" title="给Op的name加上name_scope"></a>给Op的name加上name_scope</h4><p><code>variable_scope</code>包含了<code>name_scope</code>的全部功能，即在<code>variable_scope</code>下也可以给<code>Op</code>与<code>Tensor</code>加上<code>name_scope</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'abc'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    a = tf.constant(<span class="number">1</span>, name=<span class="string">'const'</span>)</span><br><span class="line">    print(a.name)  <span class="comment"># &gt;&gt; abc/const:0</span></span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>：默认的<code>variable_scope</code>的<code>name</code>等于其对应的<code>name_scope</code>的<code>name</code>，但并不总是这样。我们可以通过如下方法查看其<code>variable_scope</code>的<code>scope_name</code>与<code>name_scope</code>的<code>scope_name</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">g = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g.as_default():</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'abc'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">      	<span class="comment"># 输出variable_scope的`name`</span></span><br><span class="line">        print(scope.name)  <span class="comment"># &gt;&gt;&gt; abc</span></span><br><span class="line">        </span><br><span class="line">        n_scope = g.get_name_scope()</span><br><span class="line">        <span class="comment"># 输出name_scope的`name`</span></span><br><span class="line">        print(n_scope)  <span class="comment"># &gt;&gt;&gt; abc</span></span><br></pre></td></tr></table></figure>
<h4 id="同名variable-scope"><a href="#同名variable-scope" class="headerlink" title="同名variable_scope"></a>同名variable_scope</h4><p>创建两个或多个<code>variable_scope</code>时可以填入相同的<code>name</code>，此时相当于创建了一个<code>variable_scope</code>与两个或多个<code>name_scope</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">g = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g.as_default():</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'abc'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">        print(scope.name)  <span class="comment"># &gt;&gt; abc</span></span><br><span class="line">        n_scope = g.get_name_scope()</span><br><span class="line">        print(n_scope)  <span class="comment"># &gt;&gt; abc</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'abc'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">        print(scope.name)  <span class="comment"># &gt;&gt; abc</span></span><br><span class="line">        n_scope = g.get_name_scope()</span><br><span class="line">        print(n_scope)  <span class="comment"># &gt;&gt; abc_1</span></span><br></pre></td></tr></table></figure>
<h3 id="与get-variable-的用法"><a href="#与get-variable-的用法" class="headerlink" title="与get_variable()的用法"></a>与get_variable()的用法</h3><p><code>variable_scope</code>的最佳搭档是<code>tf.get_variable()</code>函数。一般的，我们会在<code>variable_scope</code>中使用<code>tf.get_variable()</code>创建与获取模型的变量，并且<code>variable_scope</code>为此提供了更多便利。</p>
<h4 id="独立使用get-variable"><a href="#独立使用get-variable" class="headerlink" title="独立使用get_variable()"></a>独立使用get_variable()</h4><p>与使用<code>tf.Variable()</code>不同，独立的使用（不在变量作用域中时）<code>tf.get_variable()</code>创建变量时不需要提供初始化的值，<strong>但必须提供<code>name</code>、<code>shape</code>、<code>dtype</code>，这是确定一个变量的基本要素。</strong>使用<code>tf.get_variable</code>创建变量的方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.get_variable(<span class="string">'abc'</span>, dtype=tf.float32, shape=[])</span><br></pre></td></tr></table></figure>
<p><strong>说明</strong>：没有初始化的值，并不意味着没有值，事实上它的值是随机的。使用<code>tf.Variable()</code>创建的变量，一般不需要提供<code>shape</code>、<code>dtype</code>，这是因为可以从初始化的值中推断出来，也<strong>不需要<code>name</code>，是因为默认的TensorFlow提供了自动生成<code>name</code>的方法。</strong></p>
<p><strong><code>tf.get_variable()</code>还有一个特点是必须提供独一无二的<code>name</code>在当前变量作用域下，如果提供了重名的<code>name</code>才会抛出异常，如下：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = tf.get_variable(<span class="string">'abcd'</span>, shape=[<span class="number">1</span>])</span><br><span class="line">b = tf.get_variable(<span class="string">'abcd'</span>, shape=[<span class="number">1</span>])  <span class="comment"># ValueError</span></span><br></pre></td></tr></table></figure>
<h4 id="在变量作用域中使用get-variable"><a href="#在变量作用域中使用get-variable" class="headerlink" title="在变量作用域中使用get_variable()"></a>在变量作用域中使用get_variable()</h4><p><code>variable_scope</code>对象包含一个<code>reuse</code>属性，默认的值为<code>None</code>，在这种情况下，代表<code>variable_scope</code>不是可重用的，此时，在<code>variable_scope</code>中的<code>tf.get_variable()</code>用法与上述独立使用<code>tf.get_variable()</code>用法完全一致。<code>tf.get_variable()</code>在<code>variable_scope</code>中创建的变量会被添加上<code>variable_scope</code>的<code>scope_name</code>前缀。当<code>variable_scope</code>的<code>reuse</code>属性值为<code>True</code>时，代表此<code>variable_scope</code>是可重用的，此时在<code>variable_scope</code>中的<code>tf.get_variable()</code>用法变成了利用<code>name</code>获取已存在的变量，而无法创建变量。也就是说<code>tf.get_variable()</code>的用法是随着<code>reuse</code>的状态而改变的，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'scope'</span>, reuse=<span class="keyword">None</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    <span class="comment"># 此时reuse=None，可以用来创建变量</span></span><br><span class="line">    tf.get_variable(<span class="string">'var'</span>, dtype=tf.float32, shape=[])</span><br><span class="line">	<span class="comment"># 改修reuse=True</span></span><br><span class="line">    scope.reuse_variables()</span><br><span class="line">	<span class="comment"># 此时reuse=True，可以用来获得已有变量</span></span><br><span class="line">    var = tf.get_variable(<span class="string">'var'</span>)</span><br></pre></td></tr></table></figure>
<p>上述的写法也可以写成下面类似形式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'scope'</span>, reuse=<span class="keyword">None</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    tf.get_variable(<span class="string">'var'</span>, dtype=tf.float32, shape=[])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(scope, reuse=<span class="keyword">True</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    var = tf.get_variable(<span class="string">'var'</span>)</span><br></pre></td></tr></table></figure>
<p>可以看到下面的<code>scope</code>直接使用上面生成的<code>scope</code>而生成，也就是说<code>variable_scope</code>只要是<code>name</code>一样或者使用同一个<code>scope</code>生成，那么这些<code>variable_scope</code>都是同一个<code>variable_scope</code>。</p>
<p><strong>注意</strong>：以上两种写法从<code>variable_scope</code>的角度看是等价的，但每创建一个<code>variable_scope</code>都创建了一个<code>name_scope</code>，所以上面的写法只包含一个<code>name_scope</code>，而下面的写法包含两个<code>name_scope</code>。这也是上文提到的<code>variable_scope</code>的<code>scope_name</code>与其包含的<code>name_scope</code>的<code>scope_name</code>不完全一样的原因。</p>
<p>为了便捷，<code>reuse</code>属性也可以设置为<code>tf.AUTO_REUSE</code>，这样<code>variable_scope</code>会根据情况自动判断变量的生成与获取，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Graph().as_default():</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'scope'</span>, reuse=<span class="keyword">None</span>):</span><br><span class="line">        tf.get_variable(<span class="string">'my_var_a'</span>, shape=[], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'scope'</span>, reuse=tf.AUTO_REUSE):</span><br><span class="line">        a = tf.get_variable(<span class="string">'my_var_a'</span>)  <span class="comment"># 获取变量</span></span><br><span class="line">        b = tf.get_variable(<span class="string">'my_var_b'</span>, shape=[],  dtype=tf.float32)  <span class="comment"># 生成一个变量</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        sess.run(tf.global_variables_initializer())</span><br><span class="line">        print(sess.run([a, b]))</span><br></pre></td></tr></table></figure>
<p><code>variable_scope</code>与<code>tf.get_variable</code>配合使用这样的写法在较大型模型中是非常有用的，可以使得模型变量的复用变得容易。</p>
<p><strong>小结</strong>：</p>
<ul>
<li>在变量作用域中，如其属性<code>reuse=None</code>时，<code>tf.get_variable</code>不能获得变量；</li>
<li>在变量作用域中，如其属性<code>reuse=True</code>时，<code>tf.get_variable</code>不能创建变量；</li>
<li>在变量作用域中，<code>scope.reuse_variables()</code>可以改变下文的<code>reuse</code>属性值为<code>True</code>；</li>
<li>同名的多个变量作用域所处的上下文中的名字作用域不同。</li>
</ul>
<p><strong>小练习</strong>：</p>
<blockquote>
<p>尝试实验验证上文“注意”中提到一个<code>variable_scope</code>与两个同名<code>variable_scope</code>中<code>name_scope</code>的情况。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">g = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g.as_default():</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'abc'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">        print(scope.name)  <span class="comment"># &gt;&gt; abc</span></span><br><span class="line">        n_scope = g.get_name_scope()</span><br><span class="line">        print(n_scope)  <span class="comment"># &gt;&gt; abc</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'abc'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">        print(scope.name)  <span class="comment"># &gt;&gt; abc</span></span><br><span class="line">        n_scope = g.get_name_scope()</span><br><span class="line">        print(n_scope)  <span class="comment"># &gt;&gt; abc_1</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'abc'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">        print(scope.name)</span><br><span class="line">        n_scope=g.get_name_scope()</span><br><span class="line">        print(n_scope)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'abc'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">        print(scope.name)</span><br><span class="line">        n_scope=g.get_name_scope()</span><br><span class="line">        print(n_scope)</span><br><span class="line">        </span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">    abc</span></span><br><span class="line"><span class="string">    abc</span></span><br><span class="line"><span class="string">    abc</span></span><br><span class="line"><span class="string">    abc_1</span></span><br><span class="line"><span class="string">    abc</span></span><br><span class="line"><span class="string">    abc_2</span></span><br><span class="line"><span class="string">    abc</span></span><br><span class="line"><span class="string">    abc_3</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<h3 id="多级变量作用域-这儿不想看"><a href="#多级变量作用域-这儿不想看" class="headerlink" title="多级变量作用域(这儿不想看)"></a>多级变量作用域(这儿不想看)</h3><h1 id="6-流程控制"><a href="#6-流程控制" class="headerlink" title="6.流程控制"></a>6.流程控制</h1><p>ensorFlow流程控制相关的API主要有：</p>
<ul>
<li><code>tf.identity</code></li>
<li><code>tf.tuple</code></li>
<li><code>tf.group</code></li>
<li><code>tf.no_op</code></li>
<li><code>tf.count_up_to</code></li>
<li><code>tf.cond</code></li>
<li><code>tf.case</code></li>
<li><code>tf.while_loop</code></li>
</ul>
<h3 id="比较运算Op"><a href="#比较运算Op" class="headerlink" title="比较运算Op"></a>比较运算Op</h3><p>与流程控制经常一起出现的是比较运算，比较运算常常出现在流程控制的条件中。比较运算也是一个节点。TensorFlow中实现比较运算的API包括：</p>
<ul>
<li><code>tf.equal</code>：逐元素判等，相等返回<code>True</code>，否则返回<code>False</code>，返回的张量与输入的<code>shape</code>相同，类型为<code>DT_BOOL</code>。<strong>不支持<code>==</code>运算符重载</strong>。</li>
<li><code>tf.not_equal</code>：逐元素判等，相等返回<code>False</code>，否则返回<code>True</code>，返回的张量与输入的<code>shape</code>相同，类型为<code>DT_BOOL</code>。<strong>不支持<code>!=</code>运算符重载</strong>。</li>
<li><code>tf.less</code>：逐元素判断是否<code>x&lt;y</code>，小于则返回<code>True</code>，否则返回<code>False</code>，返回的张量与输入的<code>shape</code>相同，类型为<code>DT_BOOL</code>。支持<code>&lt;</code>运算符重载。</li>
<li><code>tf.less_equal</code>：逐元素判断是否<code>x&lt;=y</code>，小于等于则返回<code>True</code>，否则返回<code>False</code>，返回的张量与输入的<code>shape</code>相同，类型为<code>DT_BOOL</code>。支持<code>&lt;=</code>运算符重载。</li>
<li><code>tf.greater</code>：逐元素判断是否<code>x&gt;y</code>，大于则返回<code>True</code>，否则返回<code>False</code>，返回的张量与输入的<code>shape</code>相同，类型为<code>DT_BOOL</code>。支持<code>&gt;</code>运算符重载。</li>
<li><code>tf.greater_equal</code>：逐元素判断是否<code>x&gt;=y</code>，大于则返回<code>True</code>，否则返回<code>False</code>，返回的张量与输入的<code>shape</code>相同，类型为<code>DT_BOOL</code>。支持<code>&gt;=</code>运算符重载。</li>
</ul>
<p><code>tf.where</code>：是一种特殊的比价运算符，他根据判别条件，分别从两个张量中抽取子元素组成新的张量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">where(condition, x=<span class="keyword">None</span>, y=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>condition， x, y 相同维度，condition是bool型值，True/False</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">where(condition）的用法</span><br></pre></td></tr></table></figure>
<p>condition是bool型值，True/False</p>
<p>返回值，是condition中元素为True对应的索引</p>
<p>看个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a = [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]]</span><br><span class="line">b = [[<span class="number">1</span>,<span class="number">0</span>,<span class="number">3</span>],[<span class="number">1</span>,<span class="number">5</span>,<span class="number">1</span>]]</span><br><span class="line">condition1 = [[<span class="keyword">True</span>,<span class="keyword">False</span>,<span class="keyword">False</span>],</span><br><span class="line">             [<span class="keyword">False</span>,<span class="keyword">True</span>,<span class="keyword">True</span>]]</span><br><span class="line">condition2 = [[<span class="keyword">True</span>,<span class="keyword">False</span>,<span class="keyword">False</span>],</span><br><span class="line">             [<span class="keyword">False</span>,<span class="keyword">True</span>,<span class="keyword">False</span>]]</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(tf.where(condition1)))</span><br><span class="line">    print(sess.run(tf.where(condition2)))</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">[[0 0]</span></span><br><span class="line"><span class="string"> [1 1]</span></span><br><span class="line"><span class="string"> [1 2]]</span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string"> [[0 0]</span></span><br><span class="line"><span class="string"> [1 1]]</span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">where(condition, x=<span class="keyword">None</span>, y=<span class="keyword">None</span>, name=<span class="keyword">None</span>)的用法</span><br></pre></td></tr></table></figure>
<p>condition， x, y 相同维度，condition是bool型值，True/False</p>
<p>返回值是对应元素，condition中元素为True的元素替换为x中的元素，为False的元素替换为y中对应元素</p>
<p>x只负责对应替换True的元素，y只负责对应替换False的元素，x，y各有分工</p>
<p>由于是替换，返回值的维度，和condition，x ， y都是相等的。</p>
<p>看个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">x = [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]]</span><br><span class="line">y = [[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>],[<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]]</span><br><span class="line">condition3 = [[<span class="keyword">True</span>,<span class="keyword">False</span>,<span class="keyword">False</span>],</span><br><span class="line">             [<span class="keyword">False</span>,<span class="keyword">True</span>,<span class="keyword">True</span>]]</span><br><span class="line">condition4 = [[<span class="keyword">True</span>,<span class="keyword">False</span>,<span class="keyword">False</span>],</span><br><span class="line">             [<span class="keyword">True</span>,<span class="keyword">True</span>,<span class="keyword">False</span>]]</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(tf.where(condition3,x,y)))</span><br><span class="line">    print(sess.run(tf.where(condition4,x,y)))  </span><br><span class="line">    </span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string"> [[ 1  8  9]</span></span><br><span class="line"><span class="string">    [10  5  6]]</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string"> [[ 1  8  9]</span></span><br><span class="line"><span class="string">    [ 4  5 12]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>：<code>tf.where</code>不传入第二、三个参数时，返回判断条件为真的每个元素的索引。</p>
<p><strong>注意</strong>：比较运算中，参与比较的元素必须具有完全相同的<code>dtype</code>，否则无法进比较。</p>
<p><strong>小练习</strong>：</p>
<p><code>smooth_l1</code>是<code>Fast RCNN</code>模型中提出的一种多变量回归的代价函数，也是目前常用的代价函数之一，其形式为：<br>$$<br>\mathrm{smooth}_{L_1}=<br>\begin{equation}</p>
<p>\left{<br>\begin{aligned}<br>0.5x^2 &amp;&amp; \mathrm{if} \left| x \right| &lt; 1 \<br>\left| x \right| - 0.5 &amp;&amp;  \mathrm{otherwise,}<br>\end{aligned}<br>\right.</p>
<p>\end{equation}<br>$$<br>请利用比较运算操作，实现一函数能对任意张量计算<code>smooth_l1</code>的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Graph().as_default() <span class="keyword">as</span> g:</span><br><span class="line">    x=tf.placeholder(dtype=tf.float32,shape=<span class="keyword">None</span>)</span><br><span class="line">    less=<span class="number">0.5</span>*tf.square(x)</span><br><span class="line">    otherwise=tf.abs(x)<span class="number">-0.5</span></span><br><span class="line">    res=tf.where(tf.abs(x)&lt;<span class="number">1</span>,less,otherwise)</span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=g) <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(res,feed_dict=&#123;x:<span class="number">2</span>&#125;))</span><br></pre></td></tr></table></figure>
<h3 id="张量拷贝"><a href="#张量拷贝" class="headerlink" title="张量拷贝"></a>张量拷贝</h3><p><code>tf.identity</code>是用于创建张量副本的虚节点，即生成一个与输入张量相同<code>shape</code>与内容（元素）的新张量，其用法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.identity(inputs, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>当<code>tf.identity</code>输入与输出的张量在同一个设备上时（例如都在一个CPU上），实际仅仅占用了一个张量内容的内存，这也是称其为虚节点的原因。但如果<code>tf.identity</code>输入与输出的张量并不在同一个设备上时，就会拷贝一份内容到新设备中。</p>
<p><strong>注意</strong>：<code>tf.Variable</code>创建变量时，变量需存储在一个设备中，就是通过<code>tf.identity</code>实现的，并且默认的创建的变量只存在与一个设备中，这样可以避免浪费存储空间。如果有需要，也可以使用<code>tf.identity</code>将变量拷贝在别的设备中。</p>
<p><strong>小练习</strong>：</p>
<p>思考：如下代码是否能够输出期望的<code>1、2、3、4、5</code>五个结果？为什么？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = tf.Variable(<span class="number">0.</span>)</span><br><span class="line">x_plus_1 = tf.assign_add(x, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.control_dependencies([x_plus_1]):</span><br><span class="line">    y = x</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        print(sess.run(y))</span><br></pre></td></tr></table></figure>
<p>之前，我们在讲控制依赖时曾提到，只有定义到控制依赖作用域下的代码才会有控制依赖的关系。上述代码中<code>y=x</code>语句的含义是将Python变量<code>y</code>指向<code>x</code>所代表的变量，所以在控制依赖作用域中并没有添加节点，所有执行<code>y</code>并不会有对应的依赖节点的执行。</p>
<p>解决上述问题的方法很简单，只需要使用<code>y=tf.identity(x)</code>替代<code>y=x</code>即可，因为<code>y=tf.identity(x)</code>是在控制依赖的作用域中定义的节点。</p>
<h3 id="张量结组"><a href="#张量结组" class="headerlink" title="张量结组"></a>张量结组</h3><p><code>tf.group</code>可以用于将一系列<code>op</code>、<code>Tensor</code>组合成为一个<code>op</code>，方便统一操作，<code>tf.group</code>没有返回值。例如我们需要在执行一个加法操作之前对所有的变量进行一个赋值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">a = tf.Variable(<span class="number">0.</span>)</span><br><span class="line">b = tf.Variable(<span class="number">0.</span>)</span><br><span class="line">c = tf.Variable(<span class="number">0.</span>)</span><br><span class="line"></span><br><span class="line">assign_a = a.assign_add(<span class="number">1.</span>)</span><br><span class="line">assign_b = b.assign_sub(<span class="number">1.</span>)</span><br><span class="line">assign_c = c.assign_add(<span class="number">2.</span>)</span><br><span class="line"></span><br><span class="line">assign_op = tf.group([assign_a, assign_b, assign_c])</span><br><span class="line">d = a + b + c</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">	sess.run(assign_op)</span><br><span class="line">    sess.run(d)  <span class="comment"># &gt;&gt;&gt; 2</span></span><br></pre></td></tr></table></figure>
<h4 id="双分支选择结构"><a href="#双分支选择结构" class="headerlink" title="双分支选择结构"></a>双分支选择结构</h4><p><code>tf.cond</code>用来构建（双分支）选择结构，类似于Python中的<code>if...else...</code>语句。由于<code>tf.cond</code>本身只是一个函数，这回使得构建的选择结构看起来很不直观。<code>tf.cond</code>介绍如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 返回某一个分支的执行结果，两个分支返回的Tensor数量必须一样多</span></span><br><span class="line"><span class="comment"># 但形状、类型等可以不一致</span></span><br><span class="line">tf.cond(</span><br><span class="line">    pred,  <span class="comment"># `shape=[]`的DT_BOOL类型的Tenor判断条件</span></span><br><span class="line">    true_fn=<span class="keyword">None</span>,  <span class="comment"># `pred`为True时执行的函数</span></span><br><span class="line">    false_fn=<span class="keyword">None</span>,  <span class="comment"># `pred`为False时执行的函数</span></span><br><span class="line">    strict=<span class="keyword">False</span>,  </span><br><span class="line">    name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p><code>tf.cond</code>的判断条件可以是张量，也可以是变量。当执行图时，每一次的执行，选择的分支均与判断条件中此时的值有关系。例如，根据条件变量值来确定执行那个分支：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cond = tf.Variable(<span class="keyword">True</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">true_fn</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.constant(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">false_fn</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.constant(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">result = tf.cond(cond, true_fn, false_fn)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(sess.run(result))  <span class="comment"># &gt;&gt;&gt; 1</span></span><br><span class="line">    sess.run(cond.assign(<span class="keyword">False</span>))</span><br><span class="line">    print(sess.run(result))  <span class="comment"># &gt;&gt;&gt; 0</span></span><br></pre></td></tr></table></figure>
<h3 id="多路分支选择结构"><a href="#多路分支选择结构" class="headerlink" title="多路分支选择结构"></a>多路分支选择结构</h3><p>例如，实现一种阶跃函数：<br>$$<br>y=</p>
<p>\begin{equation}</p>
<p>\left{<br>\begin{aligned}<br>1 &amp;&amp; x &gt; 0 \<br>0 &amp;&amp; x = 0 \<br>-1 &amp;&amp; x &lt; 0,</p>
<p>\end{aligned}<br>\right.</p>
<p>\end{equation}<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(x)</span>:</span></span><br><span class="line">    x = tf.convert_to_tensor(x)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">greater_zero_fn</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tf.constant(<span class="number">1</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">equal_zero_fn</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tf.constant(<span class="number">0</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">less_zero_fn</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tf.constant(<span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line">    less_equal = tf.cond(x &lt; <span class="number">0</span>, less_zero_fn, equal_zero_fn)</span><br><span class="line">    result = tf.cond(x &gt; <span class="number">0</span>, greater_zero_fn, <span class="keyword">lambda</span>: less_equal)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<p>上面的利用2个2路分支结构的写法较为繁琐，并不直观。</p>
<p>一般的多路分支选择结构使用<code>tf.case</code>来实现，类似于Python中的<code>if...elif...else...</code>。上述阶梯函数使用<code>tf.case</code>的实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">step_case</span><span class="params">(x)</span>:</span></span><br><span class="line">    x = tf.convert_to_tensor(x)</span><br><span class="line">    </span><br><span class="line">    case_greater = (x &gt; <span class="number">0</span>, <span class="keyword">lambda</span>: tf.constant(<span class="number">1</span>))</span><br><span class="line">    case_equal = (tf.equal(x, <span class="number">0</span>), <span class="keyword">lambda</span>: tf.constant(<span class="number">0</span>))</span><br><span class="line">    case_less = (x &lt; <span class="number">0</span>, <span class="keyword">lambda</span>: tf.constant(<span class="number">-1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> tf.case([case_greater, case_equal, case_less])</span><br></pre></td></tr></table></figure>
<h3 id="循环结构"><a href="#循环结构" class="headerlink" title="循环结构"></a>循环结构</h3><p>TensorFlow中的循环结构使用<code>tf.while_loop</code>构建，类似于Python中的<code>while</code>、<code>for</code>语句。<code>tf.while_loop</code>的基本用法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 当`cond`为`True`时执行`body`</span></span><br><span class="line">tf.while_loop(</span><br><span class="line">    cond,  <span class="comment"># 循环条件（一个函数或lambda表达式），为`True`则继续执行循环</span></span><br><span class="line">    body,  <span class="comment"># 循环执行的内容</span></span><br><span class="line">    loop_vars,  <span class="comment"># `cond`、`body`函数的参数列表</span></span><br><span class="line">    shape_invariants=<span class="keyword">None</span>,  <span class="comment"># shape不变性</span></span><br><span class="line">    parallel_iterations=<span class="number">10</span>,  <span class="comment"># 允许并行迭代的次数</span></span><br><span class="line">    back_prop=<span class="keyword">True</span>,  <span class="comment"># 是否为此循环开启反向传播</span></span><br><span class="line">    swap_memory=<span class="keyword">False</span>,  <span class="comment"># 是否允许`Tensor`在不同设备之间交换，允许的话，可以使得内存不足的设备将张量放在别的设备中</span></span><br><span class="line">    name=<span class="keyword">None</span>, </span><br><span class="line">    maximum_iterations=<span class="keyword">None</span>, </span><br><span class="line">    return_same_structure=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p><code>tf.while_loop</code>实现实现循环结构的方式比较抽象，为了说明其功能，首先我们使用Python完成一个简单循环结构：如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">i = <span class="number">0</span></span><br><span class="line">n = <span class="number">10</span></span><br><span class="line"><span class="keyword">while</span>(i &lt; n):</span><br><span class="line">    i = i + <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>上述结构使用<code>tf.while_loop</code>实现的方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cond</span><span class="params">(i, n)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> i &lt; n</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">body</span><span class="params">(i, n)</span>:</span></span><br><span class="line">    i = i + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> i, n</span><br><span class="line"></span><br><span class="line">i = tf.constant(<span class="number">0</span>)</span><br><span class="line">n = tf.constant(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">result_i, result_n = tf.while_loop(cond, body, [i, n])</span><br></pre></td></tr></table></figure>
<p>代码中定义了两个函数<code>cond</code>、<code>body</code>，分别用来判断条件与执行循环体，同时可以看到两个函数拥有相同的输入，但<code>cond</code>的输出为<code>DT_BOOL</code>类型，<code>body</code>输出与输入结构相同。循环的执行逻辑是首先把参数<code>[i, n]</code>两个张量传入<code>coord</code>判断条件是否成立，若不成立则结束循环，否则将<code>[i, n]</code>传入<code>body</code>中，循环体执行完毕之后，需要返回操作之后的所有张量，这样在第二轮的循环中就会使用第一轮<code>body</code>的返回值输入继续判断条件，执行循环体。</p>
<p><code>tf.while_loop</code>的使用有很多需要注意的地方，例如我们需要使用一个变量记录循环的次数，试想使用如下代码执行是否可行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># i用于记录训练次数</span></span><br><span class="line">i = tf.constant(<span class="number">0</span>)</span><br><span class="line">n = tf.constant(<span class="number">10</span>)</span><br><span class="line"><span class="comment"># 定义一个变量也用来记录循环次数，每次body执行，就加1</span></span><br><span class="line">run_times = tf.Variable(<span class="number">0</span>)  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cond</span><span class="params">(i, n)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> i &lt; n</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">body</span><span class="params">(i, n)</span>:</span></span><br><span class="line">    i = i + <span class="number">1</span></span><br><span class="line">    <span class="comment"># 给var增量赋值</span></span><br><span class="line">    <span class="keyword">global</span> run_times</span><br><span class="line">    run_times.assign_add(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> i, n</span><br><span class="line"></span><br><span class="line">result_i, result_n = tf.while_loop(cond, body, [i, n])</span><br></pre></td></tr></table></figure>
<p>上述代码如果在循环执行结束后，打印变量<code>var</code>的值会发现仍然是<code>0</code>，这是因为操作<code>var.assign_add(1)</code>与<code>body</code>中的其它操作没有依赖关系，所以可以加入控制依赖解决问题，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">i = tf.constant(<span class="number">0</span>)</span><br><span class="line">n = tf.constant(<span class="number">10</span>)</span><br><span class="line"><span class="comment"># 定义一个变量用来记录循环次数</span></span><br><span class="line">run_times = tf.Variable(<span class="number">0</span>)  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cond</span><span class="params">(i, n)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> i &lt; n</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">body</span><span class="params">(i, n)</span>:</span></span><br><span class="line">    <span class="comment"># 给var增量赋值</span></span><br><span class="line">    <span class="keyword">global</span> run_times</span><br><span class="line">    assign_op = run_times.assign_add(<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 添加控制依赖</span></span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([assign_op]):</span><br><span class="line">        i = i + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> i, n</span><br><span class="line"></span><br><span class="line">result_i, result_n = tf.while_loop(cond, body, [i, n])</span><br></pre></td></tr></table></figure>
<p>除此以外，还需要注意的是，<code>tf.while_loop</code>的第三个参数即<code>cond、body</code>函数的输入参数，可以输入常量、变量，但如果输入变量了，那么这意味着使用了变量的初始值作为了输入张量，也就是说变量是没有真正输入进去的。</p>
<h3 id="Python、TensorFlow中的流程控制对比-不想看"><a href="#Python、TensorFlow中的流程控制对比-不想看" class="headerlink" title="Python、TensorFlow中的流程控制对比(不想看)"></a>Python、TensorFlow中的流程控制对比(不想看)</h3><h2 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h2><ul>
<li><code>tf.tuple</code>：可以将输入的多个张量组成一个元组。</li>
<li><code>tf.no_op</code>：不做任何操作的<code>op</code>。</li>
<li><code>tf.count_up_to</code>：控制一个<code>DT_INT32</code>或<code>DT_INT64</code>类型变量随着会话执行的次数而逐次增加，直到到达设置的上限为止。</li>
</ul>
<p><a href="https://github.com/m-L-0/18a-YangZixu-2016-533/tree/master/five" target="_blank" rel="noopener">作业传送门</a></p>
<h1 id="7-基础文件操作"><a href="#7-基础文件操作" class="headerlink" title="7.基础文件操作"></a>7.基础文件操作</h1><h1 id="8-模型存取"><a href="#8-模型存取" class="headerlink" title="8.模型存取"></a>8.模型存取</h1><h3 id="图存取"><a href="#图存取" class="headerlink" title="图存取"></a>图存取</h3><p>图是由一系列Op与Tensor构成的，我们可以通过某种方法对这些Op与Tensor进行描述，在Tensorflow中这就是’图定义’<code>GraphDef</code>。图的存取本质上就是<code>GraphDef</code>的存取</p>
<h4 id="图的保存"><a href="#图的保存" class="headerlink" title="图的保存"></a>图的保存</h4><p>图的保存方法很简单，只需要将图的定义保存即可</p>
<ul>
<li><p><strong>获取图的定义</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.graph().as_default() <span class="keyword">as</span> graph:</span><br><span class="line">    v=tf.constant([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">    print(graph.as_graph_def)</span><br></pre></td></tr></table></figure>
</li>
<li><p>还可以使用绑定图的会话的<code>graph_def</code>属性来获取图的序列化后的定义，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Graph().as_default() <span class="keyword">as</span> graph:</span><br><span class="line">    v = tf.constant([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">    print(graph.as_graph_def())</span><br><span class="line">    </span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> sess:</span><br><span class="line">    sess.graph_def == graph.as_graph_def()  <span class="comment"># True</span></span><br></pre></td></tr></table></figure>
<p><strong>注意：</strong>当会话中加入Op时，<code>sess.graph_def == graph.as_graph_def()</code>不再成立。在会话中graph_def会随着Op的改变而改变。</p>
</li>
<li><p><strong>保存图的定义</strong></p>
</li>
</ul>
<p>保存图的定义有两种方法，第一种为直接将图存为文本文件。第二种为使用Tensorflow提供的专门的保存图的方法，这种方法更加便捷。</p>
<ul>
<li>方法一：直接创建一个文件保存定义</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Graph().as_default() <span class="keyword">as</span> g:</span><br><span class="line">    tf.Variable([<span class="number">1</span>, <span class="number">2</span>], name=<span class="string">'var'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.gfile.FastGFile(<span class="string">'test_model.pb'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(g.as_graph_def().SerializeToString())</span><br></pre></td></tr></table></figure>
<p><code>SerializeToString</code>是将str类型的图定义转化为二进制的proto数据。</p>
<ul>
<li><p>方法二：</p>
<p>使用Tensorflow提供的<code>tf.train.write_graph</code>进行保存。使用此方法还有一个好处，就是可以直接将图传入即可。用法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.train.write_graph(</span><br><span class="line">    graph_or_graph_def, <span class="comment"># 图或者图定义</span></span><br><span class="line">    logdir,  <span class="comment"># 存储的文件路径</span></span><br><span class="line">    name,   <span class="comment"># 存储的文件名</span></span><br><span class="line">    as_text=<span class="keyword">True</span>)  <span class="comment"># 是否作为文本存储</span></span><br></pre></td></tr></table></figure>
<p>这些参数<code>as_text</code>的值为<code>False</code>，即保存为二进制的proto数据。此方法等价于’方法一’。</p>
<p>当<code>as_text</code>值为<code>True</code>时，保存的是str类型的数据。通常推荐为<code>False</code>。</p>
</li>
</ul>
<h4 id="图的读取"><a href="#图的读取" class="headerlink" title="图的读取"></a>图的读取</h4><p>图的读取，即将保存的图的节点加载到当前的图中。当我们保存一个图之后，这个图可以再次被获取到。</p>
<p>图的获取步骤如下：</p>
<ol>
<li>从序列化的二进制文件中读取数据</li>
<li>从读取到数据中创建<code>GraphDef</code>对象</li>
<li>导入<code>GraphDef</code>对象到当前图中，创建出对应的图结构</li>
</ol>
<p>具体如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Graph().as_default() <span class="keyword">as</span> new_graph:</span><br><span class="line">    <span class="keyword">with</span> tf.gfile.FastGFile(<span class="string">'test_model.pb'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        graph_def = tf.GraphDef()</span><br><span class="line">        graph_def.ParseFromString(f.read())</span><br><span class="line">        tf.import_graph_def(graph_def)</span><br></pre></td></tr></table></figure>
<p>这里<code>ParseFromString</code>是protocal message的方法，用于将二进制的proto数据读取成<code>GraphDef</code>数据。<code>tf.import_graph_def</code>用于将一个图定义导入到当前的默认图中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.import_graph_def(</span><br><span class="line">    graph_def,  <span class="comment"># 将要导入的图的图定义</span></span><br><span class="line">    input_map=<span class="keyword">None</span>,  <span class="comment"># 替代导入的图中的Tensor</span></span><br><span class="line">    return_elements=<span class="keyword">None</span>,  <span class="comment"># 返回指定的OP或Tensor(可以使用新的变量绑定)</span></span><br><span class="line">    name=<span class="keyword">None</span>,  <span class="comment"># 被导入的图中Op与Tensor的name前缀 默认是'import'</span></span><br><span class="line">    op_dict=<span class="keyword">None</span>, </span><br><span class="line">    producer_op_list=<span class="keyword">None</span>):</span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>：当<code>input_map</code>不为None时，<code>name</code>必须不为空。</p>
<p><strong>注意</strong>：当<code>return_elements</code>返回Op时，在会话中执行返回为<code>None</code>。</p>
<p>当然了，我们也可以使用<code>tf.Graph.get_tensor_by_name</code>与<code>tf.Graph.get_operation_by_name</code>来获取Tensor与Op，但要<strong>注意加上name前缀</strong>。</p>
<p>如果图在保存时，存为文本类型的proto数据，即<code>tf.train.write_graph</code>中的参数<code>as_text</code>为<code>True</code>时，获取图的操作稍有不同。即解码时不能使用<code>graph_def.ParseFromString</code>进行解码，而需要使用<code>protobuf</code>中的<code>text_format</code>进行操作，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> google.protobuf <span class="keyword">import</span> text_format</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Graph().as_default() <span class="keyword">as</span> new_graph:</span><br><span class="line">    <span class="keyword">with</span> tf.gfile.FastGFile(<span class="string">'test_model.pb'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        graph_def = tf.GraphDef()</span><br><span class="line">        <span class="comment"># graph_def.ParseFromString(f.read())</span></span><br><span class="line">        text_format.Merge(f.read(), graph_def)</span><br><span class="line">        tf.import_graph_def(graph_def)</span><br></pre></td></tr></table></figure>
<h2 id="变量存取"><a href="#变量存取" class="headerlink" title="变量存取"></a>变量存取</h2><p>变量存储是把模型中定义的变量存储起来，不包含图结构。另一个程序使用时，首先需要重新创建图，然后将存储的变量导入进来，即模型加载。变量存储可以脱离图存储而存在。</p>
<p>变量的存储与读取，在Tensorflow中叫做检查点存取，变量保存的文件是检查点文件(checkpoint file)，扩展名一般为.ckpt。使用<code>tf.train.Saver()</code>类来操作检查点。</p>
<h4 id="变量存取-1"><a href="#变量存取-1" class="headerlink" title="变量存取"></a>变量存取</h4><p>变量是在图中定义的，但实际上是会话中存储了变量，即我们在运行图的时候，变量才会真正存在，且变量在图的运行过程中，值会发生变化，所以我们需要<strong>在会话中保存变量</strong>。保存变量的方法是<code>tf.train.Saver.save()</code>。</p>
<p>这里需要注意，通常，我们可以在图定义完成之后初始化<code>tf.train.Saver()</code>。<code>tf.train.Saver()</code>在图中的位置很重要，在其之后的变量不会被存储在当前的<code>Save</code>对象控制。</p>
<p>创建Saver对象之后，此时并不会保存变量，我们还需要指定会话运行到什么时候时再去保存变量，需要使用<code>tf.train.Saver.save()</code>进行保存。</p>
<p>例如：</p>
<h1 id="9-数据集文件操作"><a href="#9-数据集文件操作" class="headerlink" title="9.数据集文件操作"></a>9.数据集文件操作</h1><h1 id="10-eager模式"><a href="#10-eager模式" class="headerlink" title="10.eager模式"></a>10.eager模式</h1>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/TensorFlow/" rel="tag"><i class="fa fa-tag"></i> -TensorFlow</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/09/26/k-means_image compression/" rel="next" title="K-means图片压缩">
                <i class="fa fa-chevron-left"></i> K-means图片压缩
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/Head.png"
                alt="Yangzixu" />
            
              <p class="site-author-name" itemprop="name">Yangzixu</p>
              <p class="site-description motion-element" itemprop="description">小渣渣de blog</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">22</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/yangzixu666" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:578156479@qq.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                友情链接
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://oj.acm-icpc.top/" title="橙白时光OJ" target="_blank">橙白时光OJ</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.lishengpeng.top/" title="李胜鹏" target="_blank">李胜鹏</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://blog.51ac.club" title="毛俊杰" target="_blank">毛俊杰</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://gukaifeng.me/" title="看不见的博客" target="_blank">看不见的博客</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://blog.csdn.net/hebtu666" title="范天祚" target="_blank">范天祚</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-TensorFlow概述"><span class="nav-number">1.</span> <span class="nav-text">1.TensorFlow概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Tensorflow是什么"><span class="nav-number">1.0.1.</span> <span class="nav-text">Tensorflow是什么?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tensor"><span class="nav-number">1.0.2.</span> <span class="nav-text">Tensor</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flow"><span class="nav-number">1.0.3.</span> <span class="nav-text">Flow</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#编程模式"><span class="nav-number">1.0.4.</span> <span class="nav-text">编程模式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据流图"><span class="nav-number">1.0.5.</span> <span class="nav-text">数据流图</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-图与绘画"><span class="nav-number">2.</span> <span class="nav-text">2.图与绘画</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#数据流图-1"><span class="nav-number">2.0.1.</span> <span class="nav-text">数据流图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#会话"><span class="nav-number">2.0.2.</span> <span class="nav-text">会话</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#feed与fetch"><span class="nav-number">2.0.3.</span> <span class="nav-text">feed与fetch</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#节点依赖"><span class="nav-number">2.0.4.</span> <span class="nav-text">节点依赖</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#子图"><span class="nav-number">2.0.5.</span> <span class="nav-text">子图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#多个图"><span class="nav-number">2.0.6.</span> <span class="nav-text">多个图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#单机多卡计算"><span class="nav-number">2.0.7.</span> <span class="nav-text">单机多卡计算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用图与会话的优点"><span class="nav-number">2.0.8.</span> <span class="nav-text">使用图与会话的优点</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-图的边与节点"><span class="nav-number">3.</span> <span class="nav-text">3.图的边与节点</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#边-edge"><span class="nav-number">3.0.1.</span> <span class="nav-text">边(edge)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据依赖"><span class="nav-number">3.0.2.</span> <span class="nav-text">数据依赖</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#控制依赖"><span class="nav-number">3.0.3.</span> <span class="nav-text">控制依赖</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#张量的阶、形状、数据类型"><span class="nav-number">3.0.4.</span> <span class="nav-text">张量的阶、形状、数据类型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Tensor的阶"><span class="nav-number">3.0.4.1.</span> <span class="nav-text">Tensor的阶</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#节点"><span class="nav-number">3.0.5.</span> <span class="nav-text">节点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#变量"><span class="nav-number">3.0.6.</span> <span class="nav-text">变量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#张量运算"><span class="nav-number">3.0.7.</span> <span class="nav-text">张量运算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#张量常用运算"><span class="nav-number">3.0.8.</span> <span class="nav-text">张量常用运算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#张量切片与索引"><span class="nav-number">3.0.9.</span> <span class="nav-text">张量切片与索引</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-常量-变量-占位符"><span class="nav-number">4.</span> <span class="nav-text">4.常量,变量,占位符</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#常量"><span class="nav-number">4.0.1.</span> <span class="nav-text">常量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#普通常量"><span class="nav-number">4.0.2.</span> <span class="nav-text">普通常量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#序列常量"><span class="nav-number">4.0.3.</span> <span class="nav-text">序列常量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#随机数常量"><span class="nav-number">4.0.4.</span> <span class="nav-text">随机数常量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#随机数种子"><span class="nav-number">4.0.5.</span> <span class="nav-text">随机数种子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#特殊常量"><span class="nav-number">4.0.6.</span> <span class="nav-text">特殊常量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#变量-1"><span class="nav-number">4.0.7.</span> <span class="nav-text">变量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用tf-get-variable-创建变量"><span class="nav-number">4.0.8.</span> <span class="nav-text">使用tf.get_variable()创建变量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#变量初始化"><span class="nav-number">4.0.9.</span> <span class="nav-text">变量初始化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#变量赋值"><span class="nav-number">4.0.10.</span> <span class="nav-text">变量赋值</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#变量操作注意事项"><span class="nav-number">4.0.11.</span> <span class="nav-text">变量操作注意事项</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tensor、ndarray、原生数据之间的相互转化"><span class="nav-number">4.0.12.</span> <span class="nav-text">Tensor、ndarray、原生数据之间的相互转化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Tensor-转化为原生数据、ndarray"><span class="nav-number">4.0.12.1.</span> <span class="nav-text">Tensor 转化为原生数据、ndarray</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#占位符"><span class="nav-number">4.0.13.</span> <span class="nav-text">占位符</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#feed-dict"><span class="nav-number">4.0.13.1.</span> <span class="nav-text">feed_dict</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#feed-dict的更多用法"><span class="nav-number">4.0.13.2.</span> <span class="nav-text">feed_dict的更多用法</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-名字与作用域"><span class="nav-number">5.</span> <span class="nav-text">5.名字与作用域</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#name"><span class="nav-number">5.0.1.</span> <span class="nav-text">name</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Op的name命名规范"><span class="nav-number">5.0.1.1.</span> <span class="nav-text">Op的name命名规范</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Tensor的name构成"><span class="nav-number">5.0.1.2.</span> <span class="nav-text">Tensor的name构成</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Op与Tensor的默认name"><span class="nav-number">5.0.1.3.</span> <span class="nav-text">Op与Tensor的默认name</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#重复name的处理方式"><span class="nav-number">5.0.1.4.</span> <span class="nav-text">重复name的处理方式</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#不同图中相同操作name"><span class="nav-number">5.0.2.</span> <span class="nav-text">不同图中相同操作name</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#通过name获取Op与Tensor"><span class="nav-number">5.0.3.</span> <span class="nav-text">通过name获取Op与Tensor</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#name-scope"><span class="nav-number">5.0.4.</span> <span class="nav-text">name_scope</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#多个name-scope"><span class="nav-number">5.0.5.</span> <span class="nav-text">多个name_scope</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#多级name-scope"><span class="nav-number">5.0.6.</span> <span class="nav-text">多级name_scope</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#name-scope的作用范围"><span class="nav-number">5.0.6.1.</span> <span class="nav-text">name_scope的作用范围</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#注意事项"><span class="nav-number">5.0.6.2.</span> <span class="nav-text">注意事项</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#variable-scope"><span class="nav-number">5.0.7.</span> <span class="nav-text">variable_scope</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#给Op的name加上name-scope"><span class="nav-number">5.0.7.1.</span> <span class="nav-text">给Op的name加上name_scope</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#同名variable-scope"><span class="nav-number">5.0.7.2.</span> <span class="nav-text">同名variable_scope</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#与get-variable-的用法"><span class="nav-number">5.0.8.</span> <span class="nav-text">与get_variable()的用法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#独立使用get-variable"><span class="nav-number">5.0.8.1.</span> <span class="nav-text">独立使用get_variable()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#在变量作用域中使用get-variable"><span class="nav-number">5.0.8.2.</span> <span class="nav-text">在变量作用域中使用get_variable()</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#多级变量作用域-这儿不想看"><span class="nav-number">5.0.9.</span> <span class="nav-text">多级变量作用域(这儿不想看)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#6-流程控制"><span class="nav-number">6.</span> <span class="nav-text">6.流程控制</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#比较运算Op"><span class="nav-number">6.0.1.</span> <span class="nav-text">比较运算Op</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#张量拷贝"><span class="nav-number">6.0.2.</span> <span class="nav-text">张量拷贝</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#张量结组"><span class="nav-number">6.0.3.</span> <span class="nav-text">张量结组</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#双分支选择结构"><span class="nav-number">6.0.3.1.</span> <span class="nav-text">双分支选择结构</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#多路分支选择结构"><span class="nav-number">6.0.4.</span> <span class="nav-text">多路分支选择结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#循环结构"><span class="nav-number">6.0.5.</span> <span class="nav-text">循环结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Python、TensorFlow中的流程控制对比-不想看"><span class="nav-number">6.0.6.</span> <span class="nav-text">Python、TensorFlow中的流程控制对比(不想看)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#其它"><span class="nav-number">6.1.</span> <span class="nav-text">其它</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#7-基础文件操作"><span class="nav-number">7.</span> <span class="nav-text">7.基础文件操作</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#8-模型存取"><span class="nav-number">8.</span> <span class="nav-text">8.模型存取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#图存取"><span class="nav-number">8.0.1.</span> <span class="nav-text">图存取</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#图的保存"><span class="nav-number">8.0.1.1.</span> <span class="nav-text">图的保存</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#图的读取"><span class="nav-number">8.0.1.2.</span> <span class="nav-text">图的读取</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#变量存取"><span class="nav-number">8.1.</span> <span class="nav-text">变量存取</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#变量存取-1"><span class="nav-number">8.1.0.1.</span> <span class="nav-text">变量存取</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#9-数据集文件操作"><span class="nav-number">9.</span> <span class="nav-text">9.数据集文件操作</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#10-eager模式"><span class="nav-number">10.</span> <span class="nav-text">10.eager模式</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yangzixu</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">53.2k</span>
  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客<span id="busuanzi_value_site_uv"></span>人</span>
    <span class="post-meta-divider">|</span>



<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共53.2k字</span>
</div>
        







        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("vlSaDdzoXNKuEwiiOMgrWgP1-gzGzoHsz", "9xQ8oLsnQXW9Mu09H7Y1tc");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  

  

  

  
  

</body>
</html>
